{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CL_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7ZrHtR2rQGID99tOOUXvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingYannn/CatastrophicForgetting-BERT/blob/main/CL_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ThutDvrKpbJ",
        "outputId": "ee7e276b-4504-4b7b-87d2-959f099513d6"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-z47skaf_\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-z47skaf_\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.1.0.dev0 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.0.dev0) (1.18.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.0.dev0) (2020.11.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.1.0.dev0-cp36-none-any.whl size=1375533 sha256=c0d5262013feb6388d141850bef2cf48d1981ac12d7b1100b78745479b25d3de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wfes_fee/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4dzvmS0Lbok",
        "outputId": "2bae3e0f-06af-4454-80e1-dd52e42e967d"
      },
      "source": [
        "!pip install conllu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/20/39bf21e3a0304c874c40c9cec96e3f70d2ef4b1ada3585f7dbee91dc8c05/conllu-4.2.1-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oxryQiHLs__",
        "outputId": "d95ab8be-5408-428e-925d-5b0d446b59b2"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=dd99622d80822959cc7df8db245d995c1d8ee8ff0556ead2747cd6cc5055cdc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeeR8ANp5CMG",
        "outputId": "6c91fcd7-12aa-4a19-d203-d7c0ded96aa4"
      },
      "source": [
        "mkdir data/pos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data/pos’: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBB0ascCLuss",
        "outputId": "481d8f81-4cb5-4d0a-8fa5-4adefac2a495"
      },
      "source": [
        "!curl -L -o data/pos/dev.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu'\n",
        "!curl -L -o data/pos/test.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-test.conllu'\n",
        "!curl -L -o data/pos/train.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-train.conllu'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   164  100   164    0     0    307      0 --:--:-- --:--:-- --:--:--   306\n",
            "Warning: Failed to create the file data/pos/dev.txt: No such file or directory\n",
            "  0 1671k    0  1371    0     0   1425      0  0:20:00 --:--:--  0:20:00  1425\n",
            "curl: (23) Failed writing body (0 != 1371)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   165  100   165    0     0    336      0 --:--:-- --:--:-- --:--:--   336\n",
            "Warning: Failed to create the file data/pos/test.txt: No such file or \n",
            "Warning: directory\n",
            "  0 1672k    0  1371    0     0   1122      0  0:25:26  0:00:01  0:25:25  1122\n",
            "curl: (23) Failed writing body (0 != 1371)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   166  100   166    0     0    398      0 --:--:-- --:--:-- --:--:--   397\n",
            "  0 12.8M    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0Warning: Failed to create the file data/pos/train.txt: No such file or \n",
            "Warning: directory\n",
            "  0 12.8M    0  1371    0     0    894      0  4:11:06  0:00:01  4:11:05 1338k\n",
            "curl: (23) Failed writing body (0 != 1371)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5XXoX2F5sEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a46cd9-5ada-40cf-f43e-1cd4d7e8a3f6"
      },
      "source": [
        "rm -r data/cola/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data/cola/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC4uNtyr5y_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d288c92-ab55-4147-9a66-fe6e98afbc76"
      },
      "source": [
        "rm -r data/mrpc/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data/mrpc/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIqIPLtMLxY3",
        "outputId": "e0a16291-930a-426c-954e-bbca9366413b"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --data_dir='data/' --tasks='CoLA'\n",
        "!python download_glue_data.py --data_dir='data/' --tasks='MRPC'\n",
        "!python download_glue_data.py --data_dir='data/' --tasks='RTE'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-03 23:42:42--  https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8225 (8.0K) [text/plain]\n",
            "Saving to: ‘download_glue_data.py’\n",
            "\n",
            "download_glue_data. 100%[===================>]   8.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-03 23:42:42 (85.4 MB/s) - ‘download_glue_data.py’ saved [8225/8225]\n",
            "\n",
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mfKcfgmnl8x"
      },
      "source": [
        "mv data/CoLA/ data/cola/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IBDQFMSnq8D"
      },
      "source": [
        "mv data/MRPC/ data/mrpc/"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYl_Np12nud4"
      },
      "source": [
        "mv data/RTE/ data/rte/"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uNAxPmnb0vg",
        "outputId": "a211a751-6461-41a3-e310-9b7e078172e9"
      },
      "source": [
        "!python download_glue_data.py --data_dir='data/' --tasks='RTE'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIsOVfjR_TAy",
        "outputId": "b8027b52-126a-4171-e6c8-26d6814355da"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec  3 23:43:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqU99f06L_yb",
        "outputId": "647528df-9866-428b-8b80-768d9fa26e0c"
      },
      "source": [
        "!python run.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"cola.json\" \\\n",
        "  --cuda \\\n",
        "  --do_lower_case \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 22:19:00.418179: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(cuda=True, data_dir='data', device=device(type='cuda'), do_lower_case=True, eval_batch_size=32, eval_during_training=False, max_seq_length=128, model_name_or_path='bert-base-uncased', num_eval_steps=10, num_train_epochs=1, output_dir='out', seed=42, task_params={'cola': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}}, train_batch_size=32, warmup_proportion=0.1)\n",
            "12/01/2020 22:19:02 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/01/2020 22:19:02 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/01/2020 22:19:02 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/01/2020 22:19:02 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/01/2020 22:19:02 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/01/2020 22:19:03 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/01/2020 22:19:07 - INFO - transformers.modeling_utils - Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/01/2020 22:19:07 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/01/2020 22:19:15 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/01/2020 22:19:15 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:19:15 - INFO - processors - guid: train-0\n",
            "12/01/2020 22:19:15 - INFO - processors - input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:19:15 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:19:15 - INFO - processors - guid: train-1\n",
            "12/01/2020 22:19:15 - INFO - processors - input_ids: 101 2028 2062 18404 2236 3989 1998 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:19:15 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:19:15 - INFO - processors - guid: train-2\n",
            "12/01/2020 22:19:15 - INFO - processors - input_ids: 101 2028 2062 18404 2236 3989 2030 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:19:15 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:19:15 - INFO - processors - guid: train-3\n",
            "12/01/2020 22:19:15 - INFO - processors - input_ids: 101 1996 2062 2057 2817 16025 1010 1996 13675 16103 2121 2027 2131 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:19:15 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:19:15 - INFO - processors - guid: train-4\n",
            "12/01/2020 22:19:15 - INFO - processors - input_ids: 101 2154 2011 2154 1996 8866 2024 2893 14163 8024 3771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:19:15 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:19:17 - INFO - __main__ - ***** Running training *****\n",
            "12/01/2020 22:19:17 - INFO - __main__ -  Num examples = 8551\n",
            "12/01/2020 22:19:17 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/01/2020 22:19:17 - INFO - __main__ -  Total optimization steps = 268\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/268 [00:00<03:08,  1.42it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:01<03:01,  1.46it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:01<02:58,  1.49it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:02<02:55,  1.51it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:03<02:53,  1.52it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:03<02:52,  1.52it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:04<02:50,  1.53it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:05<02:51,  1.52it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:05<02:49,  1.53it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:06<02:48,  1.53it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:07<02:48,  1.53it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:07<02:47,  1.53it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:08<02:46,  1.53it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:09<02:46,  1.52it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:09<02:46,  1.52it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:10<02:45,  1.52it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:11<02:45,  1.52it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:11<02:44,  1.52it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:12<02:45,  1.51it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:13<02:44,  1.51it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:13<02:44,  1.51it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:14<02:43,  1.50it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:15<02:43,  1.50it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:15<02:42,  1.50it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:16<02:42,  1.50it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:17<02:42,  1.49it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:17<02:40,  1.50it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:18<02:40,  1.50it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:19<02:40,  1.49it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:19<02:39,  1.49it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:20<02:39,  1.48it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:21<02:38,  1.49it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:21<02:38,  1.48it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:22<02:38,  1.48it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:23<02:38,  1.47it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:23<02:37,  1.47it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:24<02:36,  1.47it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:25<02:36,  1.47it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:25<02:36,  1.46it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:26<02:35,  1.47it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:27<02:35,  1.46it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:28<02:34,  1.46it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:28<02:34,  1.46it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:29<02:33,  1.46it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:30<02:32,  1.46it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:30<02:32,  1.45it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:31<02:31,  1.46it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:32<02:31,  1.45it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:32<02:30,  1.45it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:33<02:30,  1.45it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:34<02:29,  1.45it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:34<02:29,  1.45it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:35<02:29,  1.44it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:36<02:28,  1.44it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:37<02:28,  1.43it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:37<02:27,  1.44it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:38<02:27,  1.43it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:39<02:26,  1.43it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:39<02:26,  1.43it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:40<02:26,  1.42it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:41<02:25,  1.42it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:41<02:24,  1.42it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:42<02:24,  1.42it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:43<02:24,  1.42it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:44<02:23,  1.41it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:44<02:23,  1.41it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:45<02:22,  1.41it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:46<02:21,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:46<02:21,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:47<02:21,  1.40it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:48<02:20,  1.40it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:49<02:20,  1.39it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:49<02:20,  1.39it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:50<02:19,  1.39it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:51<02:19,  1.39it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:51<02:18,  1.39it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:52<02:18,  1.38it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:53<02:17,  1.38it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:54<02:17,  1.38it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:54<02:16,  1.37it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:55<02:16,  1.37it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:56<02:15,  1.37it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:57<02:15,  1.37it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:57<02:14,  1.37it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:58<02:13,  1.37it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [00:59<02:13,  1.37it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [00:59<02:12,  1.37it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [01:00<02:11,  1.37it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [01:01<02:10,  1.37it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [01:02<02:09,  1.37it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [01:02<02:08,  1.37it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [01:03<02:07,  1.38it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [01:04<02:07,  1.38it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [01:05<02:06,  1.38it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [01:05<02:05,  1.38it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [01:06<02:04,  1.38it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [01:07<02:03,  1.39it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [01:07<02:02,  1.39it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [01:08<02:01,  1.39it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [01:09<02:00,  1.40it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [01:10<01:59,  1.40it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [01:10<01:58,  1.40it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [01:11<01:57,  1.40it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [01:12<01:57,  1.40it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [01:12<01:56,  1.40it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [01:13<01:54,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [01:14<01:54,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [01:15<01:53,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [01:15<01:52,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [01:16<01:51,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [01:17<01:51,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [01:17<01:50,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [01:18<01:49,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [01:19<01:48,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [01:20<01:47,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [01:20<01:47,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [01:21<01:46,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [01:22<01:45,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [01:22<01:44,  1.42it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [01:23<01:43,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [01:24<01:43,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [01:24<01:42,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [01:25<01:41,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [01:26<01:40,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [01:27<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [01:27<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [01:28<01:38,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [01:29<01:37,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [01:29<01:36,  1.44it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [01:30<01:35,  1.44it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [01:31<01:35,  1.44it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [01:31<01:34,  1.44it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [01:32<01:33,  1.44it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [01:33<01:33,  1.44it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [01:33<01:32,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [01:34<01:31,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [01:35<01:31,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [01:36<01:30,  1.44it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [01:36<01:29,  1.44it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [01:37<01:28,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [01:38<01:28,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [01:38<01:27,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [01:39<01:26,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [01:40<01:26,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [01:40<01:25,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [01:41<01:24,  1.44it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [01:42<01:23,  1.44it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [01:42<01:23,  1.44it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [01:43<01:22,  1.44it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [01:44<01:22,  1.44it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [01:45<01:21,  1.44it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [01:45<01:20,  1.44it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [01:46<01:19,  1.44it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [01:47<01:19,  1.44it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [01:47<01:18,  1.43it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [01:48<01:18,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:49<01:17,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:49<01:16,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:50<01:16,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:51<01:15,  1.44it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:52<01:14,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:52<01:13,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:53<01:13,  1.44it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:54<01:12,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:54<01:12,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:55<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:56<01:10,  1.43it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:56<01:10,  1.43it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:57<01:09,  1.42it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:58<01:08,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:59<01:08,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [01:59<01:07,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [02:00<01:06,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [02:01<01:05,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [02:01<01:05,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [02:02<01:04,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [02:03<01:03,  1.42it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [02:03<01:03,  1.42it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [02:04<01:02,  1.42it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [02:05<01:01,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [02:06<01:01,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [02:06<01:00,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [02:07<00:59,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [02:08<00:59,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [02:08<00:58,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [02:09<00:57,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [02:10<00:57,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [02:11<00:56,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [02:11<00:55,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [02:12<00:55,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [02:13<00:54,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [02:13<00:53,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [02:14<00:53,  1.40it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [02:15<00:52,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [02:15<00:51,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [02:16<00:50,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [02:17<00:50,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [02:18<00:49,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [02:18<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [02:19<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [02:20<00:47,  1.41it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [02:20<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [02:21<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [02:22<00:45,  1.41it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [02:23<00:44,  1.41it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [02:23<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [02:24<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [02:25<00:42,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [02:25<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [02:26<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [02:27<00:40,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [02:28<00:39,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [02:28<00:38,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [02:29<00:38,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [02:30<00:37,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [02:30<00:36,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [02:31<00:36,  1.42it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [02:32<00:35,  1.42it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [02:32<00:34,  1.42it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [02:33<00:33,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [02:34<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [02:35<00:32,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [02:35<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [02:36<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [02:37<00:30,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [02:37<00:29,  1.42it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [02:38<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [02:39<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [02:40<00:27,  1.42it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [02:40<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [02:41<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  87% 232/268 [02:42<00:25,  1.42it/s]\u001b[A\n",
            "Iteration:  87% 233/268 [02:42<00:24,  1.42it/s]\u001b[A\n",
            "Iteration:  87% 234/268 [02:43<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 235/268 [02:44<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [02:44<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [02:45<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [02:46<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [02:47<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [02:47<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [02:48<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [02:49<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [02:49<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [02:50<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [02:51<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [02:51<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [02:52<00:14,  1.43it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [02:53<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [02:54<00:13,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [02:54<00:12,  1.43it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [02:55<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [02:56<00:11,  1.43it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [02:56<00:10,  1.43it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [02:57<00:09,  1.43it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [02:58<00:09,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [02:59<00:08,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [02:59<00:07,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [03:00<00:07,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [03:01<00:06,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [03:01<00:05,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [03:02<00:04,  1.43it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [03:03<00:04,  1.43it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [03:03<00:03,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [03:04<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [03:05<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [03:06<00:01,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [03:06<00:00,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [03:06<00:00,  1.43it/s]\n",
            "12/01/2020 22:22:24 - INFO - __main__ - ***** Average training loss: 0.50 *****\n",
            "12/01/2020 22:22:24 - INFO - __main__ - ***** Training epoch took: 0:03:07 *****\n",
            "Epoch: 100% 1/1 [03:06<00:00, 186.93s/it]\n",
            "12/01/2020 22:22:24 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/01/2020 22:22:24 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:22:24 - INFO - processors - guid: dev-0\n",
            "12/01/2020 22:22:24 - INFO - processors - input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:22:24 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:22:24 - INFO - processors - guid: dev-1\n",
            "12/01/2020 22:22:24 - INFO - processors - input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:22:24 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:22:24 - INFO - processors - guid: dev-2\n",
            "12/01/2020 22:22:24 - INFO - processors - input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:22:24 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:22:24 - INFO - processors - guid: dev-3\n",
            "12/01/2020 22:22:24 - INFO - processors - input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - label: 1 (id = 1)\n",
            "12/01/2020 22:22:24 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:22:24 - INFO - processors - guid: dev-4\n",
            "12/01/2020 22:22:24 - INFO - processors - input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:22:24 - INFO - processors - label: 0 (id = 0)\n",
            "12/01/2020 22:22:24 - INFO - __main__ - ***** Running evaluation:: Task : cola, Prefix : Current Task *****\n",
            "12/01/2020 22:22:24 - INFO - __main__ -   Num examples = 1043\n",
            "12/01/2020 22:22:24 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  4.00it/s]\n",
            "12/01/2020 22:22:32 - INFO - __main__ - ***** Eval results Current Task cola*****\n",
            "12/01/2020 22:22:32 - INFO - __main__ -  mcc = 0.5248466844154049\n",
            "12/01/2020 22:22:34 - INFO - __main__ -  global_step = 268, average loss = 0.5006866708620271\n",
            "\n",
            "***** Accuracy Matrix *****\n",
            "\n",
            "[[0.5248467]]\n",
            "\n",
            "***** Transfer Matrix *****\n",
            "Future Transfer => Upper Triangular Matrix  ||  Backward Transfer => Lower Triangular Matrix\n",
            "\n",
            "[[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqvoHIxYO6EG",
        "outputId": "d562a3ef-6671-4f7c-e8e0-27338033c115"
      },
      "source": [
        "!python run.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"ner.json\" \\\n",
        "  --cuda \\\n",
        "  --do_lower_case \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 22:11:18.286486: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(cuda=True, data_dir='data', device=device(type='cuda'), do_lower_case=True, eval_batch_size=32, eval_during_training=False, max_seq_length=128, model_name_or_path='bert-base-uncased', num_eval_steps=10, num_train_epochs=1, output_dir='out', seed=42, task_params={'ner': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}}, train_batch_size=32, warmup_proportion=0.1)\n",
            "12/01/2020 22:11:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/01/2020 22:11:20 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/01/2020 22:11:20 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/01/2020 22:11:20 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/01/2020 22:11:20 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/01/2020 22:11:21 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/01/2020 22:11:24 - INFO - transformers.modeling_utils - Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/01/2020 22:11:24 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/01/2020 22:11:32 - INFO - __main__ - Creating features from dataset file at data\n",
            "-100\n",
            "12/01/2020 22:11:32 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:11:32 - INFO - processors - guid: train-1\n",
            "12/01/2020 22:11:32 - INFO - processors - tokens: [CLS] eu rejects german call to boycott british lamb . [SEP]\n",
            "12/01/2020 22:11:32 - INFO - processors - input_ids: 101 7327 19164 2446 2655 2000 17757 2329 12559 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - label_ids: -100 5 0 1 0 0 0 1 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:11:32 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:11:32 - INFO - processors - guid: train-2\n",
            "12/01/2020 22:11:32 - INFO - processors - tokens: [CLS] peter blackburn [SEP]\n",
            "12/01/2020 22:11:32 - INFO - processors - input_ids: 101 2848 13934 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - label_ids: -100 3 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:11:32 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:11:32 - INFO - processors - guid: train-3\n",
            "12/01/2020 22:11:32 - INFO - processors - tokens: [CLS] brussels 1996 - 08 - 22 [SEP]\n",
            "12/01/2020 22:11:32 - INFO - processors - input_ids: 101 9371 2727 1011 5511 1011 2570 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - label_ids: -100 7 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:11:32 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:11:32 - INFO - processors - guid: train-4\n",
            "12/01/2020 22:11:32 - INFO - processors - tokens: [CLS] the european commission said on thursday it disagreed with german advice to consumers to shu ##n british lamb until scientists determine whether mad cow disease can be transmitted to sheep . [SEP]\n",
            "12/01/2020 22:11:32 - INFO - processors - input_ids: 101 1996 2647 3222 2056 2006 9432 2009 18335 2007 2446 6040 2000 10390 2000 18454 2078 2329 12559 2127 6529 5646 3251 5506 11190 4295 2064 2022 11860 2000 8351 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - label_ids: -100 0 5 6 0 0 0 0 0 0 1 0 0 0 0 0 -100 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:11:32 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:11:32 - INFO - processors - guid: train-5\n",
            "12/01/2020 22:11:32 - INFO - processors - tokens: [CLS] germany ' s representative to the european union ' s veterinary committee werner z ##wing ##mann said on wednesday consumers should buy sheep ##me ##at from countries other than britain until the scientific advice was clearer . [SEP]\n",
            "12/01/2020 22:11:32 - INFO - processors - input_ids: 101 2762 1005 1055 4387 2000 1996 2647 2586 1005 1055 15651 2837 14121 1062 9328 5804 2056 2006 9317 10390 2323 4965 8351 4168 4017 2013 3032 2060 2084 3725 2127 1996 4045 6040 2001 24509 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:11:32 - INFO - processors - label_ids: -100 7 0 -100 0 0 0 5 6 0 -100 0 0 3 4 -100 -100 0 0 0 0 0 0 0 -100 -100 0 0 0 0 7 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:11:43 - INFO - __main__ - ***** Running training *****\n",
            "12/01/2020 22:11:43 - INFO - __main__ -  Num examples = 14041\n",
            "12/01/2020 22:11:43 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/01/2020 22:11:43 - INFO - __main__ -  Total optimization steps = 439\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/439 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/439 [00:00<04:56,  1.48it/s]\u001b[A\n",
            "Iteration:   0% 2/439 [00:01<04:46,  1.52it/s]\u001b[A\n",
            "Iteration:   1% 3/439 [00:01<04:40,  1.55it/s]\u001b[A\n",
            "Iteration:   1% 4/439 [00:02<04:34,  1.58it/s]\u001b[A\n",
            "Iteration:   1% 5/439 [00:03<04:31,  1.60it/s]\u001b[A\n",
            "Iteration:   1% 6/439 [00:03<04:28,  1.61it/s]\u001b[A\n",
            "Iteration:   2% 7/439 [00:04<04:27,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 8/439 [00:04<04:26,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 9/439 [00:05<04:25,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 10/439 [00:06<04:24,  1.62it/s]\u001b[A\n",
            "Iteration:   3% 11/439 [00:06<04:23,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 12/439 [00:07<04:22,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 13/439 [00:08<04:22,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 14/439 [00:08<04:22,  1.62it/s]\u001b[A\n",
            "Iteration:   3% 15/439 [00:09<04:23,  1.61it/s]\u001b[A\n",
            "Iteration:   4% 16/439 [00:09<04:23,  1.61it/s]\u001b[A\n",
            "Iteration:   4% 17/439 [00:10<04:21,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 18/439 [00:11<04:21,  1.61it/s]\u001b[A\n",
            "Iteration:   4% 19/439 [00:11<04:19,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 20/439 [00:12<04:19,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 21/439 [00:12<04:18,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 22/439 [00:13<04:18,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 23/439 [00:14<04:17,  1.61it/s]\u001b[A\n",
            "Iteration:   5% 24/439 [00:14<04:17,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 25/439 [00:15<04:16,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 26/439 [00:16<04:17,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 27/439 [00:16<04:16,  1.60it/s]\u001b[A\n",
            "Iteration:   6% 28/439 [00:17<04:15,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 29/439 [00:17<04:15,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 30/439 [00:18<04:14,  1.60it/s]\u001b[A\n",
            "Iteration:   7% 31/439 [00:19<04:14,  1.60it/s]\u001b[A\n",
            "Iteration:   7% 32/439 [00:19<04:14,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 33/439 [00:20<04:14,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 34/439 [00:21<04:14,  1.59it/s]\u001b[A\n",
            "Iteration:   8% 35/439 [00:21<04:13,  1.59it/s]\u001b[A\n",
            "Iteration:   8% 36/439 [00:22<04:13,  1.59it/s]\u001b[A\n",
            "Iteration:   8% 37/439 [00:22<04:12,  1.59it/s]\u001b[A\n",
            "Iteration:   9% 38/439 [00:23<04:12,  1.59it/s]\u001b[A\n",
            "Iteration:   9% 39/439 [00:24<04:12,  1.58it/s]\u001b[A\n",
            "Iteration:   9% 40/439 [00:24<04:12,  1.58it/s]\u001b[A\n",
            "Iteration:   9% 41/439 [00:25<04:12,  1.58it/s]\u001b[A\n",
            "Iteration:  10% 42/439 [00:26<04:13,  1.57it/s]\u001b[A\n",
            "Iteration:  10% 43/439 [00:26<04:13,  1.56it/s]\u001b[A\n",
            "Iteration:  10% 44/439 [00:27<04:11,  1.57it/s]\u001b[A\n",
            "Iteration:  10% 45/439 [00:28<04:11,  1.57it/s]\u001b[A\n",
            "Iteration:  10% 46/439 [00:28<04:10,  1.57it/s]\u001b[A\n",
            "Iteration:  11% 47/439 [00:29<04:09,  1.57it/s]\u001b[A\n",
            "Iteration:  11% 48/439 [00:30<04:09,  1.57it/s]\u001b[A\n",
            "Iteration:  11% 49/439 [00:30<04:09,  1.56it/s]\u001b[A\n",
            "Iteration:  11% 50/439 [00:31<04:10,  1.56it/s]\u001b[A\n",
            "Iteration:  12% 51/439 [00:31<04:09,  1.56it/s]\u001b[A\n",
            "Iteration:  12% 52/439 [00:32<04:08,  1.56it/s]\u001b[A\n",
            "Iteration:  12% 53/439 [00:33<04:08,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 54/439 [00:33<04:08,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 55/439 [00:34<04:09,  1.54it/s]\u001b[A\n",
            "Iteration:  13% 56/439 [00:35<04:06,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 57/439 [00:35<04:05,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 58/439 [00:36<04:04,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 59/439 [00:37<04:05,  1.55it/s]\u001b[A\n",
            "Iteration:  14% 60/439 [00:37<04:04,  1.55it/s]\u001b[A\n",
            "Iteration:  14% 61/439 [00:38<04:04,  1.55it/s]\u001b[A\n",
            "Iteration:  14% 62/439 [00:39<04:04,  1.54it/s]\u001b[A\n",
            "Iteration:  14% 63/439 [00:39<04:03,  1.54it/s]\u001b[A\n",
            "Iteration:  15% 64/439 [00:40<04:03,  1.54it/s]\u001b[A\n",
            "Iteration:  15% 65/439 [00:40<04:02,  1.54it/s]\u001b[A\n",
            "Iteration:  15% 66/439 [00:41<04:03,  1.53it/s]\u001b[A\n",
            "Iteration:  15% 67/439 [00:42<04:01,  1.54it/s]\u001b[A\n",
            "Iteration:  15% 68/439 [00:42<04:02,  1.53it/s]\u001b[A\n",
            "Iteration:  16% 69/439 [00:43<04:01,  1.54it/s]\u001b[A\n",
            "Iteration:  16% 70/439 [00:44<04:00,  1.54it/s]\u001b[A\n",
            "Iteration:  16% 71/439 [00:44<03:59,  1.54it/s]\u001b[A\n",
            "Iteration:  16% 72/439 [00:45<03:58,  1.54it/s]\u001b[A\n",
            "Iteration:  17% 73/439 [00:46<03:59,  1.53it/s]\u001b[A\n",
            "Iteration:  17% 74/439 [00:46<03:58,  1.53it/s]\u001b[A\n",
            "Iteration:  17% 75/439 [00:47<03:58,  1.53it/s]\u001b[A\n",
            "Iteration:  17% 76/439 [00:48<03:58,  1.52it/s]\u001b[A\n",
            "Iteration:  18% 77/439 [00:48<03:58,  1.52it/s]\u001b[A\n",
            "Iteration:  18% 78/439 [00:49<03:58,  1.51it/s]\u001b[A\n",
            "Iteration:  18% 79/439 [00:50<03:58,  1.51it/s]\u001b[A\n",
            "Iteration:  18% 80/439 [00:50<03:57,  1.51it/s]\u001b[A\n",
            "Iteration:  18% 81/439 [00:51<03:57,  1.51it/s]\u001b[A\n",
            "Iteration:  19% 82/439 [00:52<03:57,  1.50it/s]\u001b[A\n",
            "Iteration:  19% 83/439 [00:52<03:56,  1.51it/s]\u001b[A\n",
            "Iteration:  19% 84/439 [00:53<03:56,  1.50it/s]\u001b[A\n",
            "Iteration:  19% 85/439 [00:54<03:56,  1.50it/s]\u001b[A\n",
            "Iteration:  20% 86/439 [00:54<03:54,  1.50it/s]\u001b[A\n",
            "Iteration:  20% 87/439 [00:55<03:54,  1.50it/s]\u001b[A\n",
            "Iteration:  20% 88/439 [00:56<03:53,  1.50it/s]\u001b[A\n",
            "Iteration:  20% 89/439 [00:56<03:53,  1.50it/s]\u001b[A\n",
            "Iteration:  21% 90/439 [00:57<03:52,  1.50it/s]\u001b[A\n",
            "Iteration:  21% 91/439 [00:58<03:52,  1.50it/s]\u001b[A\n",
            "Iteration:  21% 92/439 [00:58<03:52,  1.50it/s]\u001b[A\n",
            "Iteration:  21% 93/439 [00:59<03:51,  1.50it/s]\u001b[A\n",
            "Iteration:  21% 94/439 [01:00<03:50,  1.49it/s]\u001b[A\n",
            "Iteration:  22% 95/439 [01:00<03:50,  1.49it/s]\u001b[A\n",
            "Iteration:  22% 96/439 [01:01<03:50,  1.49it/s]\u001b[A\n",
            "Iteration:  22% 97/439 [01:02<03:50,  1.48it/s]\u001b[A\n",
            "Iteration:  22% 98/439 [01:02<03:50,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 99/439 [01:03<03:49,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 100/439 [01:04<03:49,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 101/439 [01:04<03:48,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 102/439 [01:05<03:48,  1.47it/s]\u001b[A\n",
            "Iteration:  23% 103/439 [01:06<03:48,  1.47it/s]\u001b[A\n",
            "Iteration:  24% 104/439 [01:06<03:47,  1.47it/s]\u001b[A\n",
            "Iteration:  24% 105/439 [01:07<03:48,  1.46it/s]\u001b[A\n",
            "Iteration:  24% 106/439 [01:08<03:46,  1.47it/s]\u001b[A\n",
            "Iteration:  24% 107/439 [01:09<03:46,  1.46it/s]\u001b[A\n",
            "Iteration:  25% 108/439 [01:09<03:46,  1.46it/s]\u001b[A\n",
            "Iteration:  25% 109/439 [01:10<03:45,  1.46it/s]\u001b[A\n",
            "Iteration:  25% 110/439 [01:11<03:47,  1.45it/s]\u001b[A\n",
            "Iteration:  25% 111/439 [01:11<03:45,  1.46it/s]\u001b[A\n",
            "Iteration:  26% 112/439 [01:12<03:44,  1.45it/s]\u001b[A\n",
            "Iteration:  26% 113/439 [01:13<03:44,  1.45it/s]\u001b[A\n",
            "Iteration:  26% 114/439 [01:13<03:44,  1.45it/s]\u001b[A\n",
            "Iteration:  26% 115/439 [01:14<03:43,  1.45it/s]\u001b[A\n",
            "Iteration:  26% 116/439 [01:15<03:42,  1.45it/s]\u001b[A\n",
            "Iteration:  27% 117/439 [01:15<03:42,  1.45it/s]\u001b[A\n",
            "Iteration:  27% 118/439 [01:16<03:41,  1.45it/s]\u001b[A\n",
            "Iteration:  27% 119/439 [01:17<03:41,  1.45it/s]\u001b[A\n",
            "Iteration:  27% 120/439 [01:18<03:40,  1.44it/s]\u001b[A\n",
            "Iteration:  28% 121/439 [01:18<03:40,  1.44it/s]\u001b[A\n",
            "Iteration:  28% 122/439 [01:19<03:40,  1.44it/s]\u001b[A\n",
            "Iteration:  28% 123/439 [01:20<03:40,  1.43it/s]\u001b[A\n",
            "Iteration:  28% 124/439 [01:20<03:39,  1.43it/s]\u001b[A\n",
            "Iteration:  28% 125/439 [01:21<03:39,  1.43it/s]\u001b[A\n",
            "Iteration:  29% 126/439 [01:22<03:39,  1.43it/s]\u001b[A\n",
            "Iteration:  29% 127/439 [01:22<03:38,  1.43it/s]\u001b[A\n",
            "Iteration:  29% 128/439 [01:23<03:37,  1.43it/s]\u001b[A\n",
            "Iteration:  29% 129/439 [01:24<03:37,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 130/439 [01:25<03:37,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 131/439 [01:25<03:36,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 132/439 [01:26<03:36,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 133/439 [01:27<03:36,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 134/439 [01:27<03:36,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 135/439 [01:28<03:36,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 136/439 [01:29<03:35,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 137/439 [01:30<03:34,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 138/439 [01:30<03:34,  1.41it/s]\u001b[A\n",
            "Iteration:  32% 139/439 [01:31<03:34,  1.40it/s]\u001b[A\n",
            "Iteration:  32% 140/439 [01:32<03:34,  1.39it/s]\u001b[A\n",
            "Iteration:  32% 141/439 [01:32<03:33,  1.40it/s]\u001b[A\n",
            "Iteration:  32% 142/439 [01:33<03:33,  1.39it/s]\u001b[A\n",
            "Iteration:  33% 143/439 [01:34<03:33,  1.38it/s]\u001b[A\n",
            "Iteration:  33% 144/439 [01:35<03:32,  1.39it/s]\u001b[A\n",
            "Iteration:  33% 145/439 [01:35<03:32,  1.38it/s]\u001b[A\n",
            "Iteration:  33% 146/439 [01:36<03:32,  1.38it/s]\u001b[A\n",
            "Iteration:  33% 147/439 [01:37<03:32,  1.38it/s]\u001b[A\n",
            "Iteration:  34% 148/439 [01:37<03:31,  1.38it/s]\u001b[A\n",
            "Iteration:  34% 149/439 [01:38<03:30,  1.38it/s]\u001b[A\n",
            "Iteration:  34% 150/439 [01:39<03:29,  1.38it/s]\u001b[A\n",
            "Iteration:  34% 151/439 [01:40<03:29,  1.37it/s]\u001b[A\n",
            "Iteration:  35% 152/439 [01:40<03:28,  1.38it/s]\u001b[A\n",
            "Iteration:  35% 153/439 [01:41<03:28,  1.37it/s]\u001b[A\n",
            "Iteration:  35% 154/439 [01:42<03:28,  1.37it/s]\u001b[A\n",
            "Iteration:  35% 155/439 [01:43<03:27,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 156/439 [01:43<03:26,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 157/439 [01:44<03:26,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 158/439 [01:45<03:25,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 159/439 [01:45<03:24,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 160/439 [01:46<03:23,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 161/439 [01:47<03:22,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 162/439 [01:48<03:21,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 163/439 [01:48<03:20,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 164/439 [01:49<03:20,  1.37it/s]\u001b[A\n",
            "Iteration:  38% 165/439 [01:50<03:19,  1.38it/s]\u001b[A\n",
            "Iteration:  38% 166/439 [01:51<03:17,  1.38it/s]\u001b[A\n",
            "Iteration:  38% 167/439 [01:51<03:17,  1.38it/s]\u001b[A\n",
            "Iteration:  38% 168/439 [01:52<03:15,  1.38it/s]\u001b[A\n",
            "Iteration:  38% 169/439 [01:53<03:14,  1.39it/s]\u001b[A\n",
            "Iteration:  39% 170/439 [01:53<03:13,  1.39it/s]\u001b[A\n",
            "Iteration:  39% 171/439 [01:54<03:12,  1.39it/s]\u001b[A\n",
            "Iteration:  39% 172/439 [01:55<03:11,  1.40it/s]\u001b[A\n",
            "Iteration:  39% 173/439 [01:56<03:10,  1.40it/s]\u001b[A\n",
            "Iteration:  40% 174/439 [01:56<03:09,  1.40it/s]\u001b[A\n",
            "Iteration:  40% 175/439 [01:57<03:08,  1.40it/s]\u001b[A\n",
            "Iteration:  40% 176/439 [01:58<03:06,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 177/439 [01:58<03:06,  1.40it/s]\u001b[A\n",
            "Iteration:  41% 178/439 [01:59<03:05,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 179/439 [02:00<03:04,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 180/439 [02:01<03:03,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 181/439 [02:01<03:02,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 182/439 [02:02<03:02,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 183/439 [02:03<03:01,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 184/439 [02:03<03:00,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 185/439 [02:04<02:59,  1.41it/s]\u001b[A\n",
            "Iteration:  42% 186/439 [02:05<02:59,  1.41it/s]\u001b[A\n",
            "Iteration:  43% 187/439 [02:06<02:58,  1.41it/s]\u001b[A\n",
            "Iteration:  43% 188/439 [02:06<02:57,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 189/439 [02:07<02:56,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 190/439 [02:08<02:55,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 191/439 [02:08<02:54,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 192/439 [02:09<02:53,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 193/439 [02:10<02:53,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 194/439 [02:10<02:52,  1.42it/s]\u001b[A\n",
            "Iteration:  44% 195/439 [02:11<02:51,  1.42it/s]\u001b[A\n",
            "Iteration:  45% 196/439 [02:12<02:50,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 197/439 [02:13<02:49,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 198/439 [02:13<02:48,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 199/439 [02:14<02:48,  1.42it/s]\u001b[A\n",
            "Iteration:  46% 200/439 [02:15<02:47,  1.42it/s]\u001b[A\n",
            "Iteration:  46% 201/439 [02:15<02:47,  1.42it/s]\u001b[A\n",
            "Iteration:  46% 202/439 [02:16<02:45,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 203/439 [02:17<02:45,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 204/439 [02:17<02:44,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 205/439 [02:18<02:43,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 206/439 [02:19<02:43,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 207/439 [02:20<02:42,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 208/439 [02:20<02:41,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 209/439 [02:21<02:40,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 210/439 [02:22<02:40,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 211/439 [02:22<02:39,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 212/439 [02:23<02:38,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 213/439 [02:24<02:38,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 214/439 [02:24<02:37,  1.42it/s]\u001b[A\n",
            "Iteration:  49% 215/439 [02:25<02:36,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 216/439 [02:26<02:35,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 217/439 [02:27<02:34,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 218/439 [02:27<02:34,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 219/439 [02:28<02:33,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 220/439 [02:29<02:33,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 221/439 [02:29<02:31,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 222/439 [02:30<02:30,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 223/439 [02:31<02:30,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 224/439 [02:31<02:29,  1.43it/s]\u001b[A\n",
            "Iteration:  51% 225/439 [02:32<02:29,  1.43it/s]\u001b[A\n",
            "Iteration:  51% 226/439 [02:33<02:28,  1.43it/s]\u001b[A\n",
            "Iteration:  52% 227/439 [02:33<02:27,  1.43it/s]\u001b[A\n",
            "Iteration:  52% 228/439 [02:34<02:27,  1.43it/s]\u001b[A\n",
            "Iteration:  52% 229/439 [02:35<02:27,  1.42it/s]\u001b[A\n",
            "Iteration:  52% 230/439 [02:36<02:26,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 231/439 [02:36<02:26,  1.42it/s]\u001b[A\n",
            "Iteration:  53% 232/439 [02:37<02:25,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 233/439 [02:38<02:24,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 234/439 [02:38<02:23,  1.42it/s]\u001b[A\n",
            "Iteration:  54% 235/439 [02:39<02:23,  1.43it/s]\u001b[A\n",
            "Iteration:  54% 236/439 [02:40<02:22,  1.42it/s]\u001b[A\n",
            "Iteration:  54% 237/439 [02:41<02:21,  1.42it/s]\u001b[A\n",
            "Iteration:  54% 238/439 [02:41<02:20,  1.43it/s]\u001b[A\n",
            "Iteration:  54% 239/439 [02:42<02:20,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 240/439 [02:43<02:19,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 241/439 [02:43<02:19,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 242/439 [02:44<02:18,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 243/439 [02:45<02:18,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 244/439 [02:45<02:17,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 245/439 [02:46<02:17,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 246/439 [02:47<02:16,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 247/439 [02:48<02:15,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 248/439 [02:48<02:14,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 249/439 [02:49<02:14,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 250/439 [02:50<02:13,  1.41it/s]\u001b[A\n",
            "Iteration:  57% 251/439 [02:50<02:13,  1.41it/s]\u001b[A\n",
            "Iteration:  57% 252/439 [02:51<02:12,  1.41it/s]\u001b[A\n",
            "Iteration:  58% 253/439 [02:52<02:11,  1.41it/s]\u001b[A\n",
            "Iteration:  58% 254/439 [02:53<02:11,  1.41it/s]\u001b[A\n",
            "Iteration:  58% 255/439 [02:53<02:10,  1.41it/s]\u001b[A\n",
            "Iteration:  58% 256/439 [02:54<02:10,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 257/439 [02:55<02:09,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 258/439 [02:55<02:08,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 259/439 [02:56<02:07,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 260/439 [02:57<02:06,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 261/439 [02:57<02:06,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 262/439 [02:58<02:05,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 263/439 [02:59<02:04,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 264/439 [03:00<02:04,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 265/439 [03:00<02:03,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 266/439 [03:01<02:02,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 267/439 [03:02<02:02,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 268/439 [03:02<02:01,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 269/439 [03:03<02:00,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 270/439 [03:04<02:00,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 271/439 [03:05<01:59,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 272/439 [03:05<01:58,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 273/439 [03:06<01:57,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 274/439 [03:07<01:57,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 275/439 [03:07<01:56,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 276/439 [03:08<01:56,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 277/439 [03:09<01:55,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 278/439 [03:10<01:54,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 279/439 [03:10<01:53,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 280/439 [03:11<01:52,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 281/439 [03:12<01:52,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 282/439 [03:12<01:51,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 283/439 [03:13<01:50,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 284/439 [03:14<01:49,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 285/439 [03:15<01:49,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 286/439 [03:15<01:48,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 287/439 [03:16<01:47,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 288/439 [03:17<01:47,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 289/439 [03:17<01:46,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 290/439 [03:18<01:45,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 291/439 [03:19<01:44,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 292/439 [03:19<01:43,  1.42it/s]\u001b[A\n",
            "Iteration:  67% 293/439 [03:20<01:43,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 294/439 [03:21<01:42,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 295/439 [03:22<01:41,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 296/439 [03:22<01:41,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 297/439 [03:23<01:40,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 298/439 [03:24<01:39,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 299/439 [03:24<01:38,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 300/439 [03:25<01:37,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 301/439 [03:26<01:37,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 302/439 [03:27<01:36,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 303/439 [03:27<01:36,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 304/439 [03:28<01:35,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 305/439 [03:29<01:34,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 306/439 [03:29<01:34,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 307/439 [03:30<01:33,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 308/439 [03:31<01:32,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 309/439 [03:31<01:31,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 310/439 [03:32<01:31,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 311/439 [03:33<01:30,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 312/439 [03:34<01:29,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 313/439 [03:34<01:28,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 314/439 [03:35<01:28,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 315/439 [03:36<01:27,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 316/439 [03:36<01:26,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 317/439 [03:37<01:25,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 318/439 [03:38<01:25,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 319/439 [03:39<01:24,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 320/439 [03:39<01:24,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 321/439 [03:40<01:23,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 322/439 [03:41<01:22,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 323/439 [03:41<01:22,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 324/439 [03:42<01:21,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 325/439 [03:43<01:20,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 326/439 [03:43<01:19,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 327/439 [03:44<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 328/439 [03:45<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 329/439 [03:46<01:17,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 330/439 [03:46<01:16,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 331/439 [03:47<01:15,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 332/439 [03:48<01:15,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 333/439 [03:48<01:14,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 334/439 [03:49<01:13,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 335/439 [03:50<01:13,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 336/439 [03:51<01:12,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 337/439 [03:51<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 338/439 [03:52<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 339/439 [03:53<01:10,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 340/439 [03:53<01:09,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 341/439 [03:54<01:09,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 342/439 [03:55<01:08,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 343/439 [03:55<01:07,  1.42it/s]\u001b[A\n",
            "Iteration:  78% 344/439 [03:56<01:07,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 345/439 [03:57<01:06,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 346/439 [03:58<01:05,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 347/439 [03:58<01:04,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 348/439 [03:59<01:04,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 349/439 [04:00<01:03,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 350/439 [04:00<01:02,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 351/439 [04:01<01:02,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 352/439 [04:02<01:01,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 353/439 [04:03<01:00,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 354/439 [04:03<01:00,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 355/439 [04:04<00:59,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 356/439 [04:05<00:58,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 357/439 [04:05<00:58,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 358/439 [04:06<00:57,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 359/439 [04:07<00:56,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 360/439 [04:07<00:55,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 361/439 [04:08<00:55,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 362/439 [04:09<00:54,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 363/439 [04:10<00:53,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 364/439 [04:10<00:53,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 365/439 [04:11<00:52,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 366/439 [04:12<00:51,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 367/439 [04:12<00:51,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 368/439 [04:13<00:50,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 369/439 [04:14<00:49,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 370/439 [04:15<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 371/439 [04:15<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 372/439 [04:16<00:47,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 373/439 [04:17<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 374/439 [04:17<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 375/439 [04:18<00:45,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 376/439 [04:19<00:44,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 377/439 [04:20<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 378/439 [04:20<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 379/439 [04:21<00:42,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 380/439 [04:22<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 381/439 [04:22<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 382/439 [04:23<00:40,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 383/439 [04:24<00:39,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 384/439 [04:25<00:39,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 385/439 [04:25<00:38,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 386/439 [04:26<00:37,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 387/439 [04:27<00:36,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 388/439 [04:27<00:36,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 389/439 [04:28<00:35,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 390/439 [04:29<00:34,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 391/439 [04:29<00:34,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 392/439 [04:30<00:33,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 393/439 [04:31<00:32,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 394/439 [04:32<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 395/439 [04:32<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 396/439 [04:33<00:30,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 397/439 [04:34<00:29,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 398/439 [04:34<00:28,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 399/439 [04:35<00:28,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 400/439 [04:36<00:27,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 401/439 [04:37<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  92% 402/439 [04:37<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  92% 403/439 [04:38<00:25,  1.40it/s]\u001b[A\n",
            "Iteration:  92% 404/439 [04:39<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  92% 405/439 [04:39<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  92% 406/439 [04:40<00:23,  1.41it/s]\u001b[A\n",
            "Iteration:  93% 407/439 [04:41<00:22,  1.41it/s]\u001b[A\n",
            "Iteration:  93% 408/439 [04:42<00:22,  1.41it/s]\u001b[A\n",
            "Iteration:  93% 409/439 [04:42<00:21,  1.41it/s]\u001b[A\n",
            "Iteration:  93% 410/439 [04:43<00:20,  1.41it/s]\u001b[A\n",
            "Iteration:  94% 411/439 [04:44<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  94% 412/439 [04:44<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  94% 413/439 [04:45<00:18,  1.41it/s]\u001b[A\n",
            "Iteration:  94% 414/439 [04:46<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 415/439 [04:47<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 416/439 [04:47<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 417/439 [04:48<00:15,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 418/439 [04:49<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 419/439 [04:49<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  96% 420/439 [04:50<00:13,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 421/439 [04:51<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 422/439 [04:51<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 423/439 [04:52<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 424/439 [04:53<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 425/439 [04:54<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 426/439 [04:54<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 427/439 [04:55<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 428/439 [04:56<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 429/439 [04:56<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 430/439 [04:57<00:06,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 431/439 [04:58<00:05,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 432/439 [04:58<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 433/439 [04:59<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 434/439 [05:00<00:03,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 435/439 [05:01<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 436/439 [05:01<00:02,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 437/439 [05:02<00:01,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 438/439 [05:03<00:00,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 439/439 [05:03<00:00,  1.45it/s]\n",
            "12/01/2020 22:16:47 - INFO - __main__ - ***** Average training loss: 0.20 *****\n",
            "12/01/2020 22:16:47 - INFO - __main__ - ***** Training epoch took: 0:05:04 *****\n",
            "Epoch: 100% 1/1 [05:03<00:00, 303.80s/it]\n",
            "12/01/2020 22:16:47 - INFO - __main__ - Creating features from dataset file at data\n",
            "-100\n",
            "12/01/2020 22:16:47 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:16:47 - INFO - processors - guid: dev-1\n",
            "12/01/2020 22:16:47 - INFO - processors - tokens: [CLS] cricket - leicestershire take over at top after innings victory . [SEP]\n",
            "12/01/2020 22:16:47 - INFO - processors - input_ids: 101 4533 1011 20034 2202 2058 2012 2327 2044 7202 3377 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - label_ids: -100 0 0 5 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:16:47 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:16:47 - INFO - processors - guid: dev-2\n",
            "12/01/2020 22:16:47 - INFO - processors - tokens: [CLS] london 1996 - 08 - 30 [SEP]\n",
            "12/01/2020 22:16:47 - INFO - processors - input_ids: 101 2414 2727 1011 5511 1011 2382 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - label_ids: -100 7 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:16:47 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:16:47 - INFO - processors - guid: dev-3\n",
            "12/01/2020 22:16:47 - INFO - processors - tokens: [CLS] west indian all - round ##er phil simmons took four for 38 on friday as leicestershire beat somerset by an innings and 39 runs in two days to take over at the head of the county championship . [SEP]\n",
            "12/01/2020 22:16:47 - INFO - processors - input_ids: 101 2225 2796 2035 1011 2461 2121 6316 13672 2165 2176 2005 4229 2006 5958 2004 20034 3786 9198 2011 2019 7202 1998 4464 3216 1999 2048 2420 2000 2202 2058 2012 1996 2132 1997 1996 2221 2528 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - label_ids: -100 1 2 0 -100 -100 -100 3 4 0 0 0 0 0 0 0 5 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:16:47 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:16:47 - INFO - processors - guid: dev-4\n",
            "12/01/2020 22:16:47 - INFO - processors - tokens: [CLS] their stay on top , though , may be short - lived as title rivals essex , derbyshire and surrey all closed in on victory while kent made up for lost time in their rain - affected match against nottinghamshire . [SEP]\n",
            "12/01/2020 22:16:47 - INFO - processors - input_ids: 101 2037 2994 2006 2327 1010 2295 1010 2089 2022 2460 1011 2973 2004 2516 9169 8862 1010 15207 1998 9948 2035 2701 1999 2006 3377 2096 5982 2081 2039 2005 2439 2051 1999 2037 4542 1011 5360 2674 2114 20126 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - label_ids: -100 0 0 0 0 0 0 0 0 0 0 -100 -100 0 0 0 5 0 5 0 5 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 -100 -100 0 0 5 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:16:47 - INFO - processors - *** Example ***\n",
            "12/01/2020 22:16:47 - INFO - processors - guid: dev-5\n",
            "12/01/2020 22:16:47 - INFO - processors - tokens: [CLS] after bowling somerset out for 83 on the opening morning at grace road , leicestershire extended their first innings by 94 runs before being bowled out for 296 with england disc ##ard andy cad ##dick taking three for 83 . [SEP]\n",
            "12/01/2020 22:16:47 - INFO - processors - input_ids: 101 2044 9116 9198 2041 2005 6640 2006 1996 3098 2851 2012 4519 2346 1010 20034 3668 2037 2034 7202 2011 6365 3216 2077 2108 19831 2041 2005 27200 2007 2563 5860 4232 5557 28353 24066 2635 2093 2005 6640 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/01/2020 22:16:47 - INFO - processors - label_ids: -100 0 0 5 0 0 0 0 0 0 0 0 7 8 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 -100 3 4 -100 0 0 0 0 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/01/2020 22:16:50 - INFO - __main__ - ***** Running evaluation:: Task : ner, Prefix : Current Task *****\n",
            "12/01/2020 22:16:50 - INFO - __main__ -   Num examples = 3250\n",
            "12/01/2020 22:16:50 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 102/102 [00:25<00:00,  3.94it/s]\n",
            "{'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8}\n",
            "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n",
            "-100\n",
            "-100\n",
            "0\n",
            "0\n",
            "Traceback (most recent call last):\n",
            "  File \"run.py\", line 445, in <module>\n",
            "    main()\n",
            "  File \"run.py\", line 422, in main\n",
            "    new_args, train_dataset, tasks, i, models[i][1], tokenizers[tasks[i]], accuracy_matrix\n",
            "  File \"run.py\", line 204, in train\n",
            "    \"Current Task\")\n",
            "  File \"run.py\", line 296, in evaluate\n",
            "    print(label_map[out_label_ids[i][j]])\n",
            "KeyError: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsQJeOF319Ht",
        "outputId": "9496fff3-7ce1-4ebf-c6d6-bea7a69ada94"
      },
      "source": [
        "!python run_ner.py \\\n",
        "  --data_dir \"data/ner\" \\\n",
        "  --do_lower_case \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"output\" \\\n",
        "  --per_gpu_train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42\\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'run_ner.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joBIBridPtXR"
      },
      "source": [
        "rm -r output2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Momo945U2OJP",
        "outputId": "c1c3bf45-a392-42ba-c464-a9c7be8e1d07"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 13.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.4MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/df/6c160e21a5caa800de16f2aa859b92671623118b4d124639aeab06876c06/boto3-1.16.28-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/e0/966b82eb9eab5fe36e80bcbbfda0d3f49cdd9ec896ebe9edb9824f896cd7/botocore-1.19.28-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.28->boto3->transformers==2.8.0) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.19.28 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.9.4\n",
            "    Uninstalling tokenizers-0.9.4:\n",
            "      Successfully uninstalled tokenizers-0.9.4\n",
            "  Found existing installation: transformers 4.1.0.dev0\n",
            "    Uninstalling transformers-4.1.0.dev0:\n",
            "      Successfully uninstalled transformers-4.1.0.dev0\n",
            "Successfully installed boto3-1.16.28 botocore-1.19.28 jmespath-0.10.0 s3transfer-0.3.3 sentencepiece-0.1.94 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ7XTJbT2xdw",
        "outputId": "25513c54-164e-4333-94bd-1024d54b0548"
      },
      "source": [
        "!python run.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"order1.json\" \\\n",
        "  --cuda \\\n",
        "  --do_lower_case \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "I-MISC I-ORG\n",
            "I-MISC O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG I-ORG\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-LOC\n",
            "I-PER I-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "I-MISC I-ORG\n",
            "I-MISC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG I-ORG\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC I-ORG\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "I-MISC B-LOC\n",
            "I-MISC I-ORG\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER O\n",
            "I-PER B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC B-MISC\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-MISC\n",
            "O I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "B-MISC O\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "B-ORG O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG I-ORG\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC O\n",
            "I-LOC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-LOC\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-PER\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-LOC\n",
            "O B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC O\n",
            "I-LOC O\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "I-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-LOC\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "I-LOC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC O\n",
            "I-LOC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-LOC\n",
            "I-ORG O\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC B-MISC\n",
            "I-MISC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "B-MISC I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER O\n",
            "I-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "I-ORG I-ORG\n",
            "I-ORG B-LOC\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "I-ORG O\n",
            "I-ORG O\n",
            "B-PER I-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC O\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-PER I-PER\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "I-MISC I-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O B-ORG\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "I-MISC B-LOC\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-MISC B-ORG\n",
            "I-MISC B-LOC\n",
            "I-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-ORG B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-PER\n",
            "I-ORG I-PER\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "I-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-MISC B-LOC\n",
            "B-MISC I-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "B-PER B-ORG\n",
            "O O\n",
            "O O\n",
            "B-PER B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC B-MISC\n",
            "O O\n",
            "B-MISC O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-PER B-PER\n",
            "I-PER I-PER\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "B-ORG B-LOC\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "B-LOC B-LOC\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "B-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "I-ORG B-ORG\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "O O\n",
            "B-MISC O\n",
            "12/04/2020 00:33:24 - INFO - __main__ - ***** Eval results Current Task ner*****\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  acc = 0.9431327573589002\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  classification report =               precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.56      0.76      0.65      7140\n",
            "        MISC       0.54      0.19      0.28      3438\n",
            "         ORG       0.42      0.53      0.47      6321\n",
            "         PER       0.87      0.92      0.89      6600\n",
            "\n",
            "   micro avg       0.60      0.66      0.63     23499\n",
            "   macro avg       0.60      0.60      0.57     23499\n",
            "weighted avg       0.61      0.66      0.61     23499\n",
            "\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  f1 = 0.6280343607882768\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  loss = 0.192045356424903\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  precision = 0.598090545118571\n",
            "12/04/2020 00:33:24 - INFO - __main__ -  recall = 0.6611345163623984\n",
            "12/04/2020 00:33:26 - INFO - __main__ -  global_step = 108, average loss = 0.4721800421399099\n",
            "\n",
            "***** Accuracy Matrix *****\n",
            "\n",
            "[[0.9431328]]\n",
            "\n",
            "***** Transfer Matrix *****\n",
            "Future Transfer => Upper Triangular Matrix  ||  Backward Transfer => Lower Triangular Matrix\n",
            "\n",
            "[[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9A51CHLedAY"
      },
      "source": [
        "rm -r out/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulRhIkLf6NKh",
        "outputId": "1871082b-68cb-492d-85ab-98101e4e0de7"
      },
      "source": [
        "!python run.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"order1.json\" \\\n",
        "  --cuda \\\n",
        "  --do_lower_case \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 22:17:20.937496: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(cuda=True, data_dir='data', device=device(type='cuda'), do_lower_case=True, eval_batch_size=32, eval_during_training=False, max_seq_length=128, model_name_or_path='bert-base-uncased', num_eval_steps=10, num_train_epochs=1, output_dir='out', seed=42, task_params={'cola': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}, 'mrpc': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}, 'rte': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}}, train_batch_size=32, warmup_proportion=0.1)\n",
            "12/02/2020 22:17:22 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:22 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"0\",\n",
            "    \"1\": \"1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"0\": 0,\n",
            "    \"1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"entailment\",\n",
            "    \"1\": \"not_entailment\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"entailment\": 0,\n",
            "    \"not_entailment\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:23 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:23 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 22:17:24 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:24 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:24 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 22:17:24 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:17:24 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:17:25 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 22:17:25 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:17:29 - INFO - transformers.modeling_utils - Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:17:29 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 22:17:29 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:17:33 - INFO - transformers.modeling_utils - Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:17:33 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 22:17:33 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:17:37 - INFO - transformers.modeling_utils - Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:17:37 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 22:17:48 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:17:48 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:17:48 - INFO - processors - guid: train-0\n",
            "12/02/2020 22:17:48 - INFO - processors - input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:17:48 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:17:48 - INFO - processors - guid: train-1\n",
            "12/02/2020 22:17:48 - INFO - processors - input_ids: 101 2028 2062 18404 2236 3989 1998 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:17:48 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:17:48 - INFO - processors - guid: train-2\n",
            "12/02/2020 22:17:48 - INFO - processors - input_ids: 101 2028 2062 18404 2236 3989 2030 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:17:48 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:17:48 - INFO - processors - guid: train-3\n",
            "12/02/2020 22:17:48 - INFO - processors - input_ids: 101 1996 2062 2057 2817 16025 1010 1996 13675 16103 2121 2027 2131 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:17:48 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:17:48 - INFO - processors - guid: train-4\n",
            "12/02/2020 22:17:48 - INFO - processors - input_ids: 101 2154 2011 2154 1996 8866 2024 2893 14163 8024 3771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:17:48 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:17:50 - INFO - __main__ - ***** Running training *****\n",
            "12/02/2020 22:17:50 - INFO - __main__ -  Num examples = 8551\n",
            "12/02/2020 22:17:50 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/02/2020 22:17:50 - INFO - __main__ -  Total optimization steps = 268\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/268 [00:00<03:10,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:01<03:03,  1.45it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:01<02:59,  1.48it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:02<02:55,  1.50it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:03<02:52,  1.52it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:03<02:50,  1.53it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:04<02:49,  1.54it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:05<02:48,  1.55it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:05<02:46,  1.55it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:06<02:46,  1.55it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:07<02:46,  1.55it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:07<02:45,  1.54it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:08<02:45,  1.54it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:09<02:44,  1.54it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:09<02:44,  1.54it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:10<02:43,  1.54it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:11<02:42,  1.54it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:11<02:42,  1.54it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:12<02:41,  1.54it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:12<02:40,  1.54it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:13<02:40,  1.54it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:14<02:40,  1.53it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:14<02:40,  1.53it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:15<02:40,  1.52it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:16<02:38,  1.53it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:16<02:38,  1.53it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:17<02:38,  1.52it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:18<02:37,  1.53it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:18<02:36,  1.53it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:19<02:36,  1.52it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:20<02:35,  1.52it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:20<02:35,  1.52it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:21<02:34,  1.52it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:22<02:34,  1.52it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:22<02:33,  1.51it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:23<02:34,  1.51it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:24<02:33,  1.51it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:24<02:33,  1.50it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:25<02:32,  1.50it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:26<02:31,  1.50it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:26<02:31,  1.50it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:27<02:31,  1.49it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:28<02:30,  1.50it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:28<02:29,  1.50it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:29<02:29,  1.50it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:30<02:28,  1.49it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:30<02:27,  1.50it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:31<02:27,  1.50it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:32<02:26,  1.49it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:32<02:26,  1.49it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:33<02:25,  1.49it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:34<02:25,  1.48it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:34<02:25,  1.48it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:35<02:24,  1.48it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:36<02:23,  1.48it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:36<02:23,  1.48it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:37<02:22,  1.48it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:38<02:22,  1.48it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:38<02:21,  1.48it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:39<02:20,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:40<02:20,  1.48it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:41<02:19,  1.48it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:41<02:18,  1.48it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:42<02:18,  1.47it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:43<02:17,  1.48it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:43<02:17,  1.47it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:44<02:16,  1.47it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:45<02:16,  1.47it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:45<02:15,  1.46it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:46<02:15,  1.46it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:47<02:14,  1.46it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:47<02:14,  1.46it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:48<02:13,  1.46it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:49<02:13,  1.46it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:49<02:12,  1.46it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:50<02:11,  1.46it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:51<02:11,  1.45it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:51<02:10,  1.45it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:52<02:10,  1.45it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:53<02:09,  1.45it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:54<02:09,  1.45it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:54<02:08,  1.45it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:55<02:08,  1.44it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:56<02:07,  1.44it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:56<02:07,  1.44it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [00:57<02:06,  1.44it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [00:58<02:05,  1.44it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [00:58<02:05,  1.44it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [00:59<02:04,  1.43it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [01:00<02:04,  1.43it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [01:01<02:03,  1.43it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [01:01<02:03,  1.43it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [01:02<02:02,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [01:03<02:02,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [01:03<02:01,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [01:04<02:01,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [01:05<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [01:05<02:00,  1.41it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [01:06<01:59,  1.41it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [01:07<01:59,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [01:08<01:58,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [01:08<01:57,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [01:09<01:57,  1.41it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [01:10<01:56,  1.41it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [01:10<01:55,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [01:11<01:55,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [01:12<01:54,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [01:13<01:54,  1.40it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [01:13<01:53,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [01:14<01:52,  1.40it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [01:15<01:52,  1.40it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [01:15<01:51,  1.40it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [01:16<01:51,  1.39it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [01:17<01:50,  1.39it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [01:18<01:50,  1.39it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [01:18<01:49,  1.39it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [01:19<01:48,  1.39it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [01:20<01:48,  1.39it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [01:20<01:47,  1.39it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [01:21<01:46,  1.39it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [01:22<01:46,  1.38it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [01:23<01:45,  1.38it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [01:23<01:44,  1.38it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [01:24<01:44,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [01:25<01:43,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [01:26<01:42,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [01:26<01:42,  1.38it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [01:27<01:41,  1.38it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [01:28<01:40,  1.38it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [01:28<01:39,  1.38it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [01:29<01:38,  1.38it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [01:30<01:38,  1.39it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [01:31<01:37,  1.39it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [01:31<01:36,  1.39it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [01:32<01:35,  1.39it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [01:33<01:34,  1.40it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [01:33<01:33,  1.40it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [01:34<01:33,  1.40it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [01:35<01:32,  1.40it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [01:36<01:31,  1.40it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [01:36<01:30,  1.40it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [01:37<01:29,  1.41it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [01:38<01:28,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [01:38<01:27,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [01:39<01:27,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [01:40<01:26,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [01:41<01:25,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [01:41<01:24,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [01:42<01:23,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [01:43<01:23,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [01:43<01:22,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [01:44<01:21,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [01:45<01:21,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [01:45<01:20,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [01:46<01:19,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [01:47<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:48<01:17,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:48<01:16,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:49<01:16,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:50<01:15,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:50<01:14,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:51<01:13,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:52<01:13,  1.44it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:52<01:12,  1.44it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:53<01:11,  1.44it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:54<01:10,  1.44it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:55<01:10,  1.44it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:55<01:09,  1.44it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:56<01:08,  1.44it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:57<01:08,  1.44it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:57<01:07,  1.44it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [01:58<01:06,  1.44it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [01:59<01:06,  1.44it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [01:59<01:05,  1.44it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [02:00<01:04,  1.44it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [02:01<01:04,  1.44it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [02:01<01:03,  1.44it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [02:02<01:02,  1.44it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [02:03<01:01,  1.44it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [02:04<01:01,  1.44it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [02:04<01:00,  1.44it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [02:05<00:59,  1.44it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [02:06<00:59,  1.44it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [02:06<00:58,  1.44it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [02:07<00:57,  1.43it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [02:08<00:57,  1.44it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [02:08<00:56,  1.43it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [02:09<00:55,  1.43it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [02:10<00:55,  1.43it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [02:11<00:54,  1.43it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [02:11<00:53,  1.43it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [02:12<00:53,  1.43it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [02:13<00:52,  1.43it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [02:13<00:51,  1.43it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [02:14<00:50,  1.43it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [02:15<00:50,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [02:15<00:49,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [02:16<00:49,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [02:17<00:48,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [02:18<00:47,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [02:18<00:47,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [02:19<00:46,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [02:20<00:45,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [02:20<00:44,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [02:21<00:44,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [02:22<00:43,  1.42it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [02:23<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [02:23<00:42,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [02:24<00:42,  1.40it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [02:25<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [02:25<00:40,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [02:26<00:39,  1.40it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [02:27<00:39,  1.40it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [02:27<00:38,  1.40it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [02:28<00:37,  1.40it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [02:29<00:37,  1.40it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [02:30<00:36,  1.40it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [02:30<00:35,  1.40it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [02:31<00:35,  1.40it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [02:32<00:34,  1.40it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [02:33<00:33,  1.40it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [02:33<00:32,  1.40it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [02:34<00:32,  1.40it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [02:35<00:31,  1.40it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [02:35<00:30,  1.40it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [02:36<00:30,  1.40it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [02:37<00:29,  1.40it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [02:38<00:28,  1.40it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [02:38<00:27,  1.40it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [02:39<00:27,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [02:40<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 232/268 [02:40<00:25,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 233/268 [02:41<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 234/268 [02:42<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 235/268 [02:42<00:23,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [02:43<00:22,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [02:44<00:21,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [02:45<00:21,  1.41it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [02:45<00:20,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [02:46<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [02:47<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [02:47<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [02:48<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [02:49<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [02:50<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [02:50<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [02:51<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [02:52<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [02:52<00:13,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [02:53<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [02:54<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [02:54<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [02:55<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [02:56<00:09,  1.43it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [02:57<00:09,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [02:57<00:08,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [02:58<00:07,  1.43it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [02:59<00:06,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [02:59<00:06,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [03:00<00:05,  1.43it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [03:01<00:04,  1.43it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [03:01<00:04,  1.43it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [03:02<00:03,  1.43it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [03:03<00:02,  1.43it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [03:04<00:02,  1.43it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [03:04<00:01,  1.43it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [03:05<00:00,  1.43it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [03:05<00:00,  1.44it/s]\n",
            "12/02/2020 22:20:56 - INFO - __main__ - ***** Average training loss: 0.50 *****\n",
            "12/02/2020 22:20:56 - INFO - __main__ - ***** Training epoch took: 0:03:06 *****\n",
            "Epoch: 100% 1/1 [03:05<00:00, 185.64s/it]\n",
            "12/02/2020 22:20:56 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:20:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:20:56 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:20:56 - INFO - processors - input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:20:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:20:56 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:20:56 - INFO - processors - input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:20:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:20:56 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:20:56 - INFO - processors - input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:20:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:20:56 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:20:56 - INFO - processors - input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:20:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:20:56 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:20:56 - INFO - processors - input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:20:56 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:20:56 - INFO - __main__ - ***** Running evaluation:: Task : cola, Prefix : Current Task *****\n",
            "12/02/2020 22:20:56 - INFO - __main__ -   Num examples = 1043\n",
            "12/02/2020 22:20:56 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  4.00it/s]\n",
            "12/02/2020 22:21:04 - INFO - __main__ - ***** Eval results Current Task cola*****\n",
            "12/02/2020 22:21:04 - INFO - __main__ -  mcc = 0.5248466844154049\n",
            "12/02/2020 22:21:06 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:21:06 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:06 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:21:06 - INFO - processors - input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:21:06 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:06 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:21:06 - INFO - processors - input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:21:06 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:06 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:21:06 - INFO - processors - input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:21:06 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:06 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:21:06 - INFO - processors - input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:21:06 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:06 - INFO - processors - guid: dev-5\n",
            "12/02/2020 22:21:06 - INFO - processors - input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:06 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:21:06 - INFO - __main__ - ***** Running evaluation:: Task : mrpc, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 22:21:06 - INFO - __main__ -   Num examples = 408\n",
            "12/02/2020 22:21:06 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  3.99it/s]\n",
            "12/02/2020 22:21:09 - INFO - __main__ - ***** Eval results Future Task (Continual) mrpc*****\n",
            "12/02/2020 22:21:09 - INFO - __main__ -  acc = 0.6838235294117647\n",
            "12/02/2020 22:21:09 - INFO - __main__ -  acc_and_f1 = 0.7480253018237863\n",
            "12/02/2020 22:21:09 - INFO - __main__ -  f1 = 0.8122270742358079\n",
            "12/02/2020 22:21:09 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:21:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:09 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:21:09 - INFO - processors - input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:21:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:09 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:21:09 - INFO - processors - input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:21:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:09 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:21:09 - INFO - processors - input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 22:21:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:21:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:21:09 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:21:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:09 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:21:09 - INFO - processors - input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 22:21:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:21:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:21:09 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:21:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:09 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:21:09 - INFO - processors - input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:09 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:21:10 - INFO - __main__ - ***** Running evaluation:: Task : rte, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 22:21:10 - INFO - __main__ -   Num examples = 277\n",
            "12/02/2020 22:21:10 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.07it/s]\n",
            "12/02/2020 22:21:12 - INFO - __main__ - ***** Eval results Future Task (Continual) rte*****\n",
            "12/02/2020 22:21:12 - INFO - __main__ -  acc = 0.5306859205776173\n",
            "12/02/2020 22:21:12 - INFO - __main__ -  global_step = 268, average loss = 0.5006866708620271\n",
            "12/02/2020 22:21:12 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:21:12 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:12 - INFO - processors - guid: train-1\n",
            "12/02/2020 22:21:12 - INFO - processors - input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:21:12 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:12 - INFO - processors - guid: train-2\n",
            "12/02/2020 22:21:12 - INFO - processors - input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:21:12 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:12 - INFO - processors - guid: train-3\n",
            "12/02/2020 22:21:12 - INFO - processors - input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:21:12 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:12 - INFO - processors - guid: train-4\n",
            "12/02/2020 22:21:12 - INFO - processors - input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:21:12 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:21:12 - INFO - processors - guid: train-5\n",
            "12/02/2020 22:21:12 - INFO - processors - input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:21:12 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:21:15 - INFO - __main__ - ***** Running training *****\n",
            "12/02/2020 22:21:15 - INFO - __main__ -  Num examples = 3668\n",
            "12/02/2020 22:21:15 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/02/2020 22:21:15 - INFO - __main__ -  Total optimization steps = 115\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:25,  1.34it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:22,  1.36it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:21,  1.37it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:20,  1.37it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:19,  1.37it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:19,  1.37it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:17,  1.37it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:16,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.37it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:13,  1.36it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.36it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:11,  1.36it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:11,  1.36it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:10,  1.36it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:10,  1.36it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:09,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:08,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:07,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:07,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:19<01:06,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:05,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.34it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:04,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:22<01:03,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:02,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:02,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:01,  1.33it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:25<01:00,  1.34it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.34it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:59,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:58,  1.34it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:28<00:57,  1.34it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.34it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:31<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.35it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:36<00:48,  1.37it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:47,  1.37it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:46,  1.37it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:46,  1.37it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:39<00:45,  1.37it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:44,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:43,  1.37it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:42,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:42,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:41,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:40,  1.37it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:44<00:40,  1.37it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:39,  1.37it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:38,  1.37it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:37,  1.37it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:47<00:37,  1.37it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:36,  1.38it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:35,  1.38it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:34,  1.38it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:49<00:34,  1.38it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:33,  1.38it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:32,  1.38it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:52<00:31,  1.38it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:52<00:31,  1.38it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.38it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:29,  1.39it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:55<00:28,  1.39it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:55<00:28,  1.38it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:27,  1.38it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:57<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:57<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:58<00:25,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:24,  1.38it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [01:00<00:23,  1.38it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:00<00:23,  1.38it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:22,  1.38it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:21,  1.39it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:02<00:20,  1.38it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:03<00:20,  1.39it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.38it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:05<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:05<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:06<00:17,  1.38it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:16,  1.38it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:08<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:08<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:09<00:14,  1.37it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:10<00:13,  1.38it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:10<00:13,  1.37it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:11<00:12,  1.37it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:12<00:11,  1.37it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:13<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:13<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:14<00:09,  1.37it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:15<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:16<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:16<00:07,  1.37it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:17<00:06,  1.37it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:18<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:19<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:19<00:04,  1.37it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:20<00:03,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:21<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:21<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:22<00:01,  1.37it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:23<00:00,  1.37it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:23<00:00,  1.37it/s]\n",
            "12/02/2020 22:22:39 - INFO - __main__ - ***** Average training loss: 0.54 *****\n",
            "12/02/2020 22:22:39 - INFO - __main__ - ***** Training epoch took: 0:01:24 *****\n",
            "Epoch: 100% 1/1 [01:23<00:00, 83.88s/it]\n",
            "12/02/2020 22:22:39 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:22:39 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:39 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:22:39 - INFO - processors - input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:39 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:39 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:22:39 - INFO - processors - input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:22:39 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:39 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:22:39 - INFO - processors - input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:22:39 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:39 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:22:39 - INFO - processors - input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:39 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:39 - INFO - processors - guid: dev-5\n",
            "12/02/2020 22:22:39 - INFO - processors - input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:39 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:22:40 - INFO - __main__ - ***** Running evaluation:: Task : mrpc, Prefix : Current Task *****\n",
            "12/02/2020 22:22:40 - INFO - __main__ -   Num examples = 408\n",
            "12/02/2020 22:22:40 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  3.95it/s]\n",
            "12/02/2020 22:22:43 - INFO - __main__ - ***** Eval results Current Task mrpc*****\n",
            "12/02/2020 22:22:43 - INFO - __main__ -  acc = 0.7892156862745098\n",
            "12/02/2020 22:22:43 - INFO - __main__ -  acc_and_f1 = 0.8256975867269984\n",
            "12/02/2020 22:22:43 - INFO - __main__ -  f1 = 0.8621794871794871\n",
            "12/02/2020 22:22:44 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:22:44 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:44 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:22:44 - INFO - processors - input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:44 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:44 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:22:44 - INFO - processors - input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:44 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:44 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:22:44 - INFO - processors - input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:44 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:44 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:22:44 - INFO - processors - input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:22:44 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:44 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:22:44 - INFO - processors - input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:44 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:22:45 - INFO - __main__ - ***** Running evaluation:: Task : cola, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 22:22:45 - INFO - __main__ -   Num examples = 1043\n",
            "12/02/2020 22:22:45 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  3.96it/s]\n",
            "12/02/2020 22:22:53 - INFO - __main__ - ***** Eval results Previous Task (Continual) cola*****\n",
            "12/02/2020 22:22:53 - INFO - __main__ -  mcc = 0.4101731381565716\n",
            "12/02/2020 22:22:53 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:22:53 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:53 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:22:53 - INFO - processors - input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:22:53 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:53 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:22:53 - INFO - processors - input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:22:53 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:53 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:22:53 - INFO - processors - input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 22:22:53 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:22:53 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:22:53 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:22:53 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:53 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:22:53 - INFO - processors - input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 22:22:53 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:22:53 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:22:53 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:22:53 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:53 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:22:53 - INFO - processors - input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:53 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:22:53 - INFO - __main__ - ***** Running evaluation:: Task : rte, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 22:22:53 - INFO - __main__ -   Num examples = 277\n",
            "12/02/2020 22:22:53 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.07it/s]\n",
            "12/02/2020 22:22:56 - INFO - __main__ - ***** Eval results Future Task (Continual) rte*****\n",
            "12/02/2020 22:22:56 - INFO - __main__ -  acc = 0.5703971119133574\n",
            "12/02/2020 22:22:56 - INFO - __main__ -  global_step = 115, average loss = 0.5435596243194912\n",
            "12/02/2020 22:22:56 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:22:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:56 - INFO - processors - guid: train-0\n",
            "12/02/2020 22:22:56 - INFO - processors - input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:22:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:56 - INFO - processors - guid: train-1\n",
            "12/02/2020 22:22:56 - INFO - processors - input_ids: 101 1037 2173 1997 14038 1010 2044 4831 2198 2703 2462 2351 1010 2150 1037 2173 1997 7401 1010 2004 3142 3234 11633 5935 1999 5116 3190 2000 2928 1996 8272 1997 2047 4831 12122 16855 1012 102 4831 12122 16855 2003 1996 2047 3003 1997 1996 3142 3234 2277 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:22:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:56 - INFO - processors - guid: train-2\n",
            "12/02/2020 22:22:56 - INFO - processors - input_ids: 101 2014 3401 13876 2378 2001 2525 4844 2000 7438 1996 5305 4355 7388 4456 5022 1010 1998 1996 2194 2056 1010 6928 1010 2009 2097 6848 2007 2976 25644 1996 6061 1997 3653 11020 3089 10472 1996 4319 2005 2062 7388 4456 5022 1012 102 2014 3401 13876 2378 2064 2022 2109 2000 7438 7388 4456 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:22:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:56 - INFO - processors - guid: train-3\n",
            "12/02/2020 22:22:56 - INFO - processors - input_ids: 101 18414 10265 13801 1010 2708 3237 2012 20877 2098 5555 1010 1037 2966 2326 2194 2008 7126 15770 1996 1016 1011 2095 1011 2214 5148 2540 2820 1999 7570 9610 19538 2103 1006 3839 24001 1007 1010 2056 2008 2061 2521 2055 1015 1010 3156 2336 2031 2363 3949 1012 102 1996 3025 2171 1997 7570 9610 19538 2103 2001 24001 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:22:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:22:56 - INFO - processors - guid: train-4\n",
            "12/02/2020 22:22:56 - INFO - processors - input_ids: 101 1037 2158 2003 2349 1999 2457 2101 5338 2007 1996 4028 2656 2086 3283 1997 1037 10563 3005 2553 2001 1996 2034 2000 2022 2956 2006 4035 2028 1005 1055 4126 18866 1012 5624 4674 19027 2213 1010 2385 1010 2001 3788 2000 2014 6898 1005 1055 2160 1999 3145 5172 1010 20126 1010 2006 2382 2255 3172 2043 2016 5419 1012 2014 2303 2001 2101 2179 1999 1037 2492 2485 2000 2014 2188 1012 2703 5954 17165 1010 2753 1010 2038 2042 5338 2007 4028 1998 2003 2349 2077 11331 23007 2101 1012 102 2703 5954 17165 2003 5496 1997 2383 13263 1037 2611 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:22:56 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:22:59 - INFO - __main__ - ***** Running training *****\n",
            "12/02/2020 22:22:59 - INFO - __main__ -  Num examples = 2490\n",
            "12/02/2020 22:22:59 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/02/2020 22:22:59 - INFO - __main__ -  Total optimization steps = 78\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/78 [00:00<00:58,  1.32it/s]\u001b[A\n",
            "Iteration:   3% 2/78 [00:01<00:57,  1.33it/s]\u001b[A\n",
            "Iteration:   4% 3/78 [00:02<00:55,  1.34it/s]\u001b[A\n",
            "Iteration:   5% 4/78 [00:02<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:   6% 5/78 [00:03<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:   8% 6/78 [00:04<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:   9% 7/78 [00:05<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  10% 8/78 [00:05<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  12% 9/78 [00:06<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  13% 10/78 [00:07<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  14% 11/78 [00:08<00:49,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 12/78 [00:08<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 13/78 [00:09<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 14/78 [00:10<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 15/78 [00:11<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 16/78 [00:11<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 17/78 [00:12<00:45,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 18/78 [00:13<00:44,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 19/78 [00:14<00:44,  1.34it/s]\u001b[A\n",
            "Iteration:  26% 20/78 [00:14<00:43,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 21/78 [00:15<00:42,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 22/78 [00:16<00:41,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 23/78 [00:17<00:41,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 24/78 [00:17<00:40,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 25/78 [00:18<00:39,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 26/78 [00:19<00:39,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 27/78 [00:20<00:38,  1.33it/s]\u001b[A\n",
            "Iteration:  36% 28/78 [00:20<00:37,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 29/78 [00:21<00:36,  1.34it/s]\u001b[A\n",
            "Iteration:  38% 30/78 [00:22<00:36,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 31/78 [00:23<00:35,  1.34it/s]\u001b[A\n",
            "Iteration:  41% 32/78 [00:23<00:34,  1.34it/s]\u001b[A\n",
            "Iteration:  42% 33/78 [00:24<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 34/78 [00:25<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  45% 35/78 [00:26<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  46% 36/78 [00:26<00:31,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 37/78 [00:27<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 38/78 [00:28<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 39/78 [00:29<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 40/78 [00:29<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 41/78 [00:30<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 42/78 [00:31<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 43/78 [00:31<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  56% 44/78 [00:32<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 45/78 [00:33<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 46/78 [00:34<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 47/78 [00:34<00:22,  1.36it/s]\u001b[A\n",
            "Iteration:  62% 48/78 [00:35<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 49/78 [00:36<00:21,  1.36it/s]\u001b[A\n",
            "Iteration:  64% 50/78 [00:37<00:20,  1.36it/s]\u001b[A\n",
            "Iteration:  65% 51/78 [00:37<00:19,  1.35it/s]\u001b[A\n",
            "Iteration:  67% 52/78 [00:38<00:19,  1.36it/s]\u001b[A\n",
            "Iteration:  68% 53/78 [00:39<00:18,  1.36it/s]\u001b[A\n",
            "Iteration:  69% 54/78 [00:40<00:17,  1.36it/s]\u001b[A\n",
            "Iteration:  71% 55/78 [00:40<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  72% 56/78 [00:41<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  73% 57/78 [00:42<00:15,  1.36it/s]\u001b[A\n",
            "Iteration:  74% 58/78 [00:43<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  76% 59/78 [00:43<00:14,  1.35it/s]\u001b[A\n",
            "Iteration:  77% 60/78 [00:44<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  78% 61/78 [00:45<00:12,  1.36it/s]\u001b[A\n",
            "Iteration:  79% 62/78 [00:45<00:11,  1.36it/s]\u001b[A\n",
            "Iteration:  81% 63/78 [00:46<00:11,  1.35it/s]\u001b[A\n",
            "Iteration:  82% 64/78 [00:47<00:10,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 65/78 [00:48<00:09,  1.36it/s]\u001b[A\n",
            "Iteration:  85% 66/78 [00:48<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  86% 67/78 [00:49<00:08,  1.35it/s]\u001b[A\n",
            "Iteration:  87% 68/78 [00:50<00:07,  1.35it/s]\u001b[A\n",
            "Iteration:  88% 69/78 [00:51<00:06,  1.35it/s]\u001b[A\n",
            "Iteration:  90% 70/78 [00:51<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  91% 71/78 [00:52<00:05,  1.35it/s]\u001b[A\n",
            "Iteration:  92% 72/78 [00:53<00:04,  1.36it/s]\u001b[A\n",
            "Iteration:  94% 73/78 [00:54<00:03,  1.36it/s]\u001b[A\n",
            "Iteration:  95% 74/78 [00:54<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  96% 75/78 [00:55<00:02,  1.36it/s]\u001b[A\n",
            "Iteration:  97% 76/78 [00:56<00:01,  1.36it/s]\u001b[A\n",
            "Iteration:  99% 77/78 [00:57<00:00,  1.36it/s]\u001b[A\n",
            "Iteration: 100% 78/78 [00:57<00:00,  1.35it/s]\n",
            "12/02/2020 22:23:56 - INFO - __main__ - ***** Average training loss: 0.66 *****\n",
            "12/02/2020 22:23:56 - INFO - __main__ - ***** Training epoch took: 0:00:58 *****\n",
            "Epoch: 100% 1/1 [00:57<00:00, 57.66s/it]\n",
            "12/02/2020 22:23:56 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:23:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:23:56 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:23:56 - INFO - processors - input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:23:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:23:56 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:23:56 - INFO - processors - input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:23:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:23:56 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:23:56 - INFO - processors - input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 22:23:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:23:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:23:56 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:23:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:23:56 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:23:56 - INFO - processors - input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 22:23:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:23:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:23:56 - INFO - processors - label: not_entailment (id = 1)\n",
            "12/02/2020 22:23:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:23:56 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:23:56 - INFO - processors - input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:23:56 - INFO - processors - label: entailment (id = 0)\n",
            "12/02/2020 22:23:57 - INFO - __main__ - ***** Running evaluation:: Task : rte, Prefix : Current Task *****\n",
            "12/02/2020 22:23:57 - INFO - __main__ -   Num examples = 277\n",
            "12/02/2020 22:23:57 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.09it/s]\n",
            "12/02/2020 22:23:59 - INFO - __main__ - ***** Eval results Current Task rte*****\n",
            "12/02/2020 22:23:59 - INFO - __main__ -  acc = 0.6209386281588448\n",
            "12/02/2020 22:24:00 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:24:00 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:00 - INFO - processors - guid: dev-0\n",
            "12/02/2020 22:24:00 - INFO - processors - input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:00 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:00 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:24:00 - INFO - processors - input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:00 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:00 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:24:00 - INFO - processors - input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:00 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:00 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:24:00 - INFO - processors - input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:00 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:00 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:24:00 - INFO - processors - input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:00 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:24:01 - INFO - __main__ - ***** Running evaluation:: Task : cola, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 22:24:01 - INFO - __main__ -   Num examples = 1043\n",
            "12/02/2020 22:24:01 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  4.00it/s]\n",
            "12/02/2020 22:24:09 - INFO - __main__ - ***** Eval results Previous Task (Continual) cola*****\n",
            "12/02/2020 22:24:09 - INFO - __main__ -  mcc = 0.28710292511754365\n",
            "12/02/2020 22:24:09 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 22:24:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:09 - INFO - processors - guid: dev-1\n",
            "12/02/2020 22:24:09 - INFO - processors - input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:09 - INFO - processors - guid: dev-2\n",
            "12/02/2020 22:24:09 - INFO - processors - input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:24:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:09 - INFO - processors - guid: dev-3\n",
            "12/02/2020 22:24:09 - INFO - processors - input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:24:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:09 - INFO - processors - guid: dev-4\n",
            "12/02/2020 22:24:09 - INFO - processors - input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - label: 1 (id = 1)\n",
            "12/02/2020 22:24:09 - INFO - processors - *** Example ***\n",
            "12/02/2020 22:24:09 - INFO - processors - guid: dev-5\n",
            "12/02/2020 22:24:09 - INFO - processors - input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:24:09 - INFO - processors - label: 0 (id = 0)\n",
            "12/02/2020 22:24:09 - INFO - __main__ - ***** Running evaluation:: Task : mrpc, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 22:24:09 - INFO - __main__ -   Num examples = 408\n",
            "12/02/2020 22:24:09 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  3.98it/s]\n",
            "12/02/2020 22:24:13 - INFO - __main__ - ***** Eval results Previous Task (Continual) mrpc*****\n",
            "12/02/2020 22:24:13 - INFO - __main__ -  acc = 0.7573529411764706\n",
            "12/02/2020 22:24:13 - INFO - __main__ -  acc_and_f1 = 0.8024052841475573\n",
            "12/02/2020 22:24:13 - INFO - __main__ -  f1 = 0.847457627118644\n",
            "12/02/2020 22:24:13 - INFO - __main__ -  global_step = 78, average loss = 0.6578781551275498\n",
            "\n",
            "***** Accuracy Matrix *****\n",
            "\n",
            "[[0.5248467 0.6838235 0.5306859]\n",
            " [0.4101731 0.7892157 0.5703971]\n",
            " [0.2871029 0.7573529 0.6209386]]\n",
            "\n",
            "***** Transfer Matrix *****\n",
            "Future Transfer => Upper Triangular Matrix  ||  Backward Transfer => Lower Triangular Matrix\n",
            "\n",
            "[[ 0.        -0.1053922 -0.0902527]\n",
            " [-0.1146736  0.        -0.0505415]\n",
            " [-0.2377438 -0.0318628  0.       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSK1up8Pbtoh",
        "outputId": "671404d0-d95b-40c9-f418-eb005120279c"
      },
      "source": [
        "a = 1\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYGECohBGlmR",
        "outputId": "a93c313e-1338-4d55-e6c9-53b1ce70b157"
      },
      "source": [
        "!python run.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"pos.json\" \\\n",
        "  --cuda \\\n",
        "  --do_lower_case \\\n",
        "  --model_name_or_path \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --train_batch_size 32 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --seed 42"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 17:38:25.849805: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(cuda=True, data_dir='data', device=device(type='cuda'), do_lower_case=True, eval_batch_size=32, eval_during_training=False, max_seq_length=128, model_name_or_path='bert-base-uncased', num_eval_steps=10, num_train_epochs=1, output_dir='out', seed=42, task_params={'pos': {'learning_rate': 3e-05, 'weight_decay': 0.01, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}}, train_batch_size=32, warmup_proportion=0.1)\n",
            "12/02/2020 17:38:27 - INFO - filelock - Lock 140487265803960 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "12/02/2020 17:38:27 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmps1nowimk\n",
            "Downloading: 100% 433/433 [00:00<00:00, 380kB/s]\n",
            "12/02/2020 17:38:27 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 17:38:27 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 17:38:27 - INFO - filelock - Lock 140487265803960 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "12/02/2020 17:38:27 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 17:38:27 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 20,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"pos\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"ADJ\",\n",
            "    \"1\": \"ADP\",\n",
            "    \"10\": \"PRON\",\n",
            "    \"11\": \"PROPN\",\n",
            "    \"12\": \"PUNCT\",\n",
            "    \"13\": \"SCONJ\",\n",
            "    \"14\": \"SYM\",\n",
            "    \"15\": \"VERB\",\n",
            "    \"16\": \"[CLS]\",\n",
            "    \"17\": \"[SEP]\",\n",
            "    \"18\": \"_\",\n",
            "    \"19\": \"X\",\n",
            "    \"2\": \"ADV\",\n",
            "    \"3\": \"AUX\",\n",
            "    \"4\": \"CCONJ\",\n",
            "    \"5\": \"DET\",\n",
            "    \"6\": \"INTJ\",\n",
            "    \"7\": \"NOUN\",\n",
            "    \"8\": \"NUM\",\n",
            "    \"9\": \"PART\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"ADJ\": 0,\n",
            "    \"ADP\": 1,\n",
            "    \"ADV\": 2,\n",
            "    \"AUX\": 3,\n",
            "    \"CCONJ\": 4,\n",
            "    \"DET\": 5,\n",
            "    \"INTJ\": 6,\n",
            "    \"NOUN\": 7,\n",
            "    \"NUM\": 8,\n",
            "    \"PART\": 9,\n",
            "    \"PRON\": 10,\n",
            "    \"PROPN\": 11,\n",
            "    \"PUNCT\": 12,\n",
            "    \"SCONJ\": 13,\n",
            "    \"SYM\": 14,\n",
            "    \"VERB\": 15,\n",
            "    \"X\": 19,\n",
            "    \"[CLS]\": 16,\n",
            "    \"[SEP]\": 17,\n",
            "    \"_\": 18\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 17:38:28 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 17:38:28 - INFO - transformers.configuration_utils - Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 17:38:28 - INFO - filelock - Lock 140487265864840 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "12/02/2020 17:38:28 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp08lxd_p5\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 889kB/s]\n",
            "12/02/2020 17:38:29 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 17:38:29 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 17:38:29 - INFO - filelock - Lock 140487265864840 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "12/02/2020 17:38:29 - INFO - transformers.tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 17:38:29 - INFO - filelock - Lock 140487265917416 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "12/02/2020 17:38:29 - INFO - transformers.file_utils - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdm1yjyvq\n",
            "Downloading: 100% 440M/440M [00:12<00:00, 35.8MB/s]\n",
            "12/02/2020 17:38:41 - INFO - transformers.file_utils - storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 17:38:41 - INFO - transformers.file_utils - creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 17:38:41 - INFO - filelock - Lock 140487265917416 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "12/02/2020 17:38:41 - INFO - transformers.modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 17:38:45 - INFO - transformers.modeling_utils - Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 17:38:45 - INFO - transformers.modeling_utils - Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 17:38:52 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 17:38:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:38:56 - INFO - processors - guid: train-0\n",
            "12/02/2020 17:38:56 - INFO - processors - tokens: [CLS] al - za ##man : american forces killed sha ##ikh abdullah al - an ##i , the preacher at the mosque in the town of q ##ai ##m , near the syrian border . [SEP]\n",
            "12/02/2020 17:38:56 - INFO - processors - input_ids: 101 2632 1011 23564 2386 1024 2137 2749 2730 21146 28209 14093 2632 1011 2019 2072 1010 1996 14512 2012 1996 8806 1999 1996 2237 1997 1053 4886 2213 1010 2379 1996 9042 3675 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - label_ids: -100 11 12 11 -100 12 0 7 15 11 -100 11 11 12 11 -100 12 5 7 1 5 7 1 5 7 1 11 -100 -100 12 1 5 0 7 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:38:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:38:56 - INFO - processors - guid: train-1\n",
            "12/02/2020 17:38:56 - INFO - processors - tokens: [CLS] [ this killing of a respected cleric will be causing us trouble for years to come . ] [SEP]\n",
            "12/02/2020 17:38:56 - INFO - processors - input_ids: 101 1031 2023 4288 1997 1037 9768 29307 2097 2022 4786 2149 4390 2005 2086 2000 2272 1012 1033 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - label_ids: -100 12 5 7 1 5 0 7 3 3 15 10 7 1 7 9 15 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:38:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:38:56 - INFO - processors - guid: train-2\n",
            "12/02/2020 17:38:56 - INFO - processors - tokens: [CLS] d ##pa : iraqi authorities announced that they had busted up 3 terrorist cells operating in baghdad . [SEP]\n",
            "12/02/2020 17:38:56 - INFO - processors - input_ids: 101 1040 4502 1024 8956 4614 2623 2008 2027 2018 23142 2039 1017 9452 4442 4082 1999 13952 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - label_ids: -100 11 -100 12 0 7 15 13 10 3 15 1 8 0 7 15 1 11 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:38:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:38:56 - INFO - processors - guid: train-3\n",
            "12/02/2020 17:38:56 - INFO - processors - tokens: [CLS] two of them were being run by 2 officials of the ministry of the interior ! [SEP]\n",
            "12/02/2020 17:38:56 - INFO - processors - input_ids: 101 2048 1997 2068 2020 2108 2448 2011 1016 4584 1997 1996 3757 1997 1996 4592 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - label_ids: -100 8 1 10 3 3 15 1 8 7 1 5 11 1 5 11 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:38:56 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:38:56 - INFO - processors - guid: train-4\n",
            "12/02/2020 17:38:56 - INFO - processors - tokens: [CLS] the moi in iraq is equivalent to the us fbi , so this would be like having j . edgar hoover un ##wi ##tting ##ly employ at a high level members of the weather ##men bombers back in the 1960s . [SEP]\n",
            "12/02/2020 17:38:56 - INFO - processors - input_ids: 101 1996 25175 1999 5712 2003 5662 2000 1996 2149 8495 1010 2061 2023 2052 2022 2066 2383 1046 1012 9586 17443 4895 9148 13027 2135 12666 2012 1037 2152 2504 2372 1997 1996 4633 3549 10544 2067 1999 1996 4120 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:38:56 - INFO - processors - label_ids: -100 5 11 1 11 3 0 1 5 11 11 12 2 10 3 15 13 15 11 -100 11 11 2 -100 -100 -100 15 1 5 0 7 7 1 5 11 -100 7 2 1 5 7 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:39:07 - INFO - __main__ - ***** Running training *****\n",
            "12/02/2020 17:39:07 - INFO - __main__ -  Num examples = 12543\n",
            "12/02/2020 17:39:07 - INFO - __main__ -  Instantaneous batch size per GPU = 32\n",
            "12/02/2020 17:39:07 - INFO - __main__ -  Total optimization steps = 392\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/392 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/392 [00:00<04:34,  1.42it/s]\u001b[A\n",
            "Iteration:   1% 2/392 [00:01<04:21,  1.49it/s]\u001b[A\n",
            "Iteration:   1% 3/392 [00:01<04:14,  1.53it/s]\u001b[A\n",
            "Iteration:   1% 4/392 [00:02<04:07,  1.57it/s]\u001b[A\n",
            "Iteration:   1% 5/392 [00:03<04:04,  1.58it/s]\u001b[A\n",
            "Iteration:   2% 6/392 [00:03<04:00,  1.60it/s]\u001b[A\n",
            "Iteration:   2% 7/392 [00:04<03:58,  1.61it/s]\u001b[A\n",
            "Iteration:   2% 8/392 [00:04<03:56,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 9/392 [00:05<03:54,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 10/392 [00:06<03:53,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 11/392 [00:06<03:53,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 12/392 [00:07<03:52,  1.64it/s]\u001b[A\n",
            "Iteration:   3% 13/392 [00:08<03:51,  1.64it/s]\u001b[A\n",
            "Iteration:   4% 14/392 [00:08<03:50,  1.64it/s]\u001b[A\n",
            "Iteration:   4% 15/392 [00:09<03:49,  1.64it/s]\u001b[A\n",
            "Iteration:   4% 16/392 [00:09<03:49,  1.64it/s]\u001b[A\n",
            "Iteration:   4% 17/392 [00:10<03:49,  1.64it/s]\u001b[A\n",
            "Iteration:   5% 18/392 [00:11<03:49,  1.63it/s]\u001b[A\n",
            "Iteration:   5% 19/392 [00:11<03:48,  1.63it/s]\u001b[A\n",
            "Iteration:   5% 20/392 [00:12<03:48,  1.63it/s]\u001b[A\n",
            "Iteration:   5% 21/392 [00:12<03:47,  1.63it/s]\u001b[A\n",
            "Iteration:   6% 22/392 [00:13<03:46,  1.63it/s]\u001b[A\n",
            "Iteration:   6% 23/392 [00:14<03:46,  1.63it/s]\u001b[A\n",
            "Iteration:   6% 24/392 [00:14<03:46,  1.62it/s]\u001b[A\n",
            "Iteration:   6% 25/392 [00:15<03:46,  1.62it/s]\u001b[A\n",
            "Iteration:   7% 26/392 [00:15<03:45,  1.62it/s]\u001b[A\n",
            "Iteration:   7% 27/392 [00:16<03:45,  1.62it/s]\u001b[A\n",
            "Iteration:   7% 28/392 [00:17<03:44,  1.62it/s]\u001b[A\n",
            "Iteration:   7% 29/392 [00:17<03:44,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 30/392 [00:18<03:44,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 31/392 [00:19<03:44,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 32/392 [00:19<03:44,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 33/392 [00:20<03:43,  1.61it/s]\u001b[A\n",
            "Iteration:   9% 34/392 [00:20<03:43,  1.61it/s]\u001b[A\n",
            "Iteration:   9% 35/392 [00:21<03:42,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 36/392 [00:22<03:41,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 37/392 [00:22<03:41,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 38/392 [00:23<03:41,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 39/392 [00:24<03:40,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 40/392 [00:24<03:39,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 41/392 [00:25<03:39,  1.60it/s]\u001b[A\n",
            "Iteration:  11% 42/392 [00:25<03:38,  1.60it/s]\u001b[A\n",
            "Iteration:  11% 43/392 [00:26<03:38,  1.60it/s]\u001b[A\n",
            "Iteration:  11% 44/392 [00:27<03:37,  1.60it/s]\u001b[A\n",
            "Iteration:  11% 45/392 [00:27<03:37,  1.60it/s]\u001b[A\n",
            "Iteration:  12% 46/392 [00:28<03:36,  1.60it/s]\u001b[A\n",
            "Iteration:  12% 47/392 [00:29<03:36,  1.60it/s]\u001b[A\n",
            "Iteration:  12% 48/392 [00:29<03:35,  1.60it/s]\u001b[A\n",
            "Iteration:  12% 49/392 [00:30<03:35,  1.59it/s]\u001b[A\n",
            "Iteration:  13% 50/392 [00:30<03:34,  1.59it/s]\u001b[A\n",
            "Iteration:  13% 51/392 [00:31<03:34,  1.59it/s]\u001b[A\n",
            "Iteration:  13% 52/392 [00:32<03:34,  1.59it/s]\u001b[A\n",
            "Iteration:  14% 53/392 [00:32<03:33,  1.59it/s]\u001b[A\n",
            "Iteration:  14% 54/392 [00:33<03:33,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 55/392 [00:34<03:33,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 56/392 [00:34<03:32,  1.58it/s]\u001b[A\n",
            "Iteration:  15% 57/392 [00:35<03:33,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 58/392 [00:36<03:31,  1.58it/s]\u001b[A\n",
            "Iteration:  15% 59/392 [00:36<03:31,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 60/392 [00:37<03:31,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 61/392 [00:37<03:30,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 62/392 [00:38<03:30,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 63/392 [00:39<03:29,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 64/392 [00:39<03:28,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 65/392 [00:40<03:28,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 66/392 [00:41<03:28,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 67/392 [00:41<03:27,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 68/392 [00:42<03:27,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 69/392 [00:43<03:26,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 70/392 [00:43<03:25,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 71/392 [00:44<03:25,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 72/392 [00:45<03:25,  1.56it/s]\u001b[A\n",
            "Iteration:  19% 73/392 [00:45<03:25,  1.55it/s]\u001b[A\n",
            "Iteration:  19% 74/392 [00:46<03:25,  1.55it/s]\u001b[A\n",
            "Iteration:  19% 75/392 [00:46<03:24,  1.55it/s]\u001b[A\n",
            "Iteration:  19% 76/392 [00:47<03:23,  1.55it/s]\u001b[A\n",
            "Iteration:  20% 77/392 [00:48<03:23,  1.55it/s]\u001b[A\n",
            "Iteration:  20% 78/392 [00:48<03:23,  1.55it/s]\u001b[A\n",
            "Iteration:  20% 79/392 [00:49<03:22,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 80/392 [00:50<03:21,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 81/392 [00:50<03:21,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 82/392 [00:51<03:21,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 83/392 [00:52<03:21,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 84/392 [00:52<03:21,  1.53it/s]\u001b[A\n",
            "Iteration:  22% 85/392 [00:53<03:20,  1.53it/s]\u001b[A\n",
            "Iteration:  22% 86/392 [00:54<03:18,  1.54it/s]\u001b[A\n",
            "Iteration:  22% 87/392 [00:54<03:18,  1.54it/s]\u001b[A\n",
            "Iteration:  22% 88/392 [00:55<03:17,  1.54it/s]\u001b[A\n",
            "Iteration:  23% 89/392 [00:56<03:16,  1.54it/s]\u001b[A\n",
            "Iteration:  23% 90/392 [00:56<03:16,  1.54it/s]\u001b[A\n",
            "Iteration:  23% 91/392 [00:57<03:15,  1.54it/s]\u001b[A\n",
            "Iteration:  23% 92/392 [00:57<03:14,  1.54it/s]\u001b[A\n",
            "Iteration:  24% 93/392 [00:58<03:13,  1.54it/s]\u001b[A\n",
            "Iteration:  24% 94/392 [00:59<03:13,  1.54it/s]\u001b[A\n",
            "Iteration:  24% 95/392 [00:59<03:13,  1.53it/s]\u001b[A\n",
            "Iteration:  24% 96/392 [01:00<03:13,  1.53it/s]\u001b[A\n",
            "Iteration:  25% 97/392 [01:01<03:13,  1.53it/s]\u001b[A\n",
            "Iteration:  25% 98/392 [01:01<03:12,  1.53it/s]\u001b[A\n",
            "Iteration:  25% 99/392 [01:02<03:12,  1.52it/s]\u001b[A\n",
            "Iteration:  26% 100/392 [01:03<03:11,  1.52it/s]\u001b[A\n",
            "Iteration:  26% 101/392 [01:03<03:11,  1.52it/s]\u001b[A\n",
            "Iteration:  26% 102/392 [01:04<03:10,  1.52it/s]\u001b[A\n",
            "Iteration:  26% 103/392 [01:05<03:10,  1.52it/s]\u001b[A\n",
            "Iteration:  27% 104/392 [01:05<03:10,  1.51it/s]\u001b[A\n",
            "Iteration:  27% 105/392 [01:06<03:09,  1.51it/s]\u001b[A\n",
            "Iteration:  27% 106/392 [01:07<03:09,  1.51it/s]\u001b[A\n",
            "Iteration:  27% 107/392 [01:07<03:08,  1.51it/s]\u001b[A\n",
            "Iteration:  28% 108/392 [01:08<03:08,  1.51it/s]\u001b[A\n",
            "Iteration:  28% 109/392 [01:09<03:08,  1.50it/s]\u001b[A\n",
            "Iteration:  28% 110/392 [01:09<03:07,  1.50it/s]\u001b[A\n",
            "Iteration:  28% 111/392 [01:10<03:07,  1.50it/s]\u001b[A\n",
            "Iteration:  29% 112/392 [01:11<03:06,  1.50it/s]\u001b[A\n",
            "Iteration:  29% 113/392 [01:11<03:06,  1.50it/s]\u001b[A\n",
            "Iteration:  29% 114/392 [01:12<03:05,  1.50it/s]\u001b[A\n",
            "Iteration:  29% 115/392 [01:13<03:05,  1.50it/s]\u001b[A\n",
            "Iteration:  30% 116/392 [01:13<03:04,  1.50it/s]\u001b[A\n",
            "Iteration:  30% 117/392 [01:14<03:04,  1.49it/s]\u001b[A\n",
            "Iteration:  30% 118/392 [01:15<03:03,  1.50it/s]\u001b[A\n",
            "Iteration:  30% 119/392 [01:15<03:02,  1.49it/s]\u001b[A\n",
            "Iteration:  31% 120/392 [01:16<03:02,  1.49it/s]\u001b[A\n",
            "Iteration:  31% 121/392 [01:17<03:01,  1.50it/s]\u001b[A\n",
            "Iteration:  31% 122/392 [01:17<03:00,  1.50it/s]\u001b[A\n",
            "Iteration:  31% 123/392 [01:18<03:00,  1.49it/s]\u001b[A\n",
            "Iteration:  32% 124/392 [01:19<03:00,  1.49it/s]\u001b[A\n",
            "Iteration:  32% 125/392 [01:19<03:00,  1.48it/s]\u001b[A\n",
            "Iteration:  32% 126/392 [01:20<02:59,  1.48it/s]\u001b[A\n",
            "Iteration:  32% 127/392 [01:21<02:59,  1.48it/s]\u001b[A\n",
            "Iteration:  33% 128/392 [01:21<02:58,  1.48it/s]\u001b[A\n",
            "Iteration:  33% 129/392 [01:22<02:58,  1.48it/s]\u001b[A\n",
            "Iteration:  33% 130/392 [01:23<02:58,  1.47it/s]\u001b[A\n",
            "Iteration:  33% 131/392 [01:24<02:57,  1.47it/s]\u001b[A\n",
            "Iteration:  34% 132/392 [01:24<02:56,  1.47it/s]\u001b[A\n",
            "Iteration:  34% 133/392 [01:25<02:56,  1.47it/s]\u001b[A\n",
            "Iteration:  34% 134/392 [01:26<02:56,  1.46it/s]\u001b[A\n",
            "Iteration:  34% 135/392 [01:26<02:55,  1.46it/s]\u001b[A\n",
            "Iteration:  35% 136/392 [01:27<02:55,  1.46it/s]\u001b[A\n",
            "Iteration:  35% 137/392 [01:28<02:54,  1.46it/s]\u001b[A\n",
            "Iteration:  35% 138/392 [01:28<02:54,  1.46it/s]\u001b[A\n",
            "Iteration:  35% 139/392 [01:29<02:53,  1.46it/s]\u001b[A\n",
            "Iteration:  36% 140/392 [01:30<02:53,  1.45it/s]\u001b[A\n",
            "Iteration:  36% 141/392 [01:30<02:52,  1.45it/s]\u001b[A\n",
            "Iteration:  36% 142/392 [01:31<02:52,  1.45it/s]\u001b[A\n",
            "Iteration:  36% 143/392 [01:32<02:51,  1.45it/s]\u001b[A\n",
            "Iteration:  37% 144/392 [01:32<02:50,  1.45it/s]\u001b[A\n",
            "Iteration:  37% 145/392 [01:33<02:50,  1.45it/s]\u001b[A\n",
            "Iteration:  37% 146/392 [01:34<02:49,  1.46it/s]\u001b[A\n",
            "Iteration:  38% 147/392 [01:35<02:48,  1.45it/s]\u001b[A\n",
            "Iteration:  38% 148/392 [01:35<02:48,  1.45it/s]\u001b[A\n",
            "Iteration:  38% 149/392 [01:36<02:47,  1.45it/s]\u001b[A\n",
            "Iteration:  38% 150/392 [01:37<02:47,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 151/392 [01:37<02:46,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 152/392 [01:38<02:46,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 153/392 [01:39<02:45,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 154/392 [01:39<02:44,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 155/392 [01:40<02:44,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 156/392 [01:41<02:44,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 157/392 [01:41<02:43,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 158/392 [01:42<02:43,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 159/392 [01:43<02:42,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 160/392 [01:44<02:42,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 161/392 [01:44<02:42,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 162/392 [01:45<02:41,  1.43it/s]\u001b[A\n",
            "Iteration:  42% 163/392 [01:46<02:40,  1.42it/s]\u001b[A\n",
            "Iteration:  42% 164/392 [01:46<02:40,  1.42it/s]\u001b[A\n",
            "Iteration:  42% 165/392 [01:47<02:39,  1.42it/s]\u001b[A\n",
            "Iteration:  42% 166/392 [01:48<02:39,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 167/392 [01:49<02:38,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 168/392 [01:49<02:38,  1.41it/s]\u001b[A\n",
            "Iteration:  43% 169/392 [01:50<02:37,  1.41it/s]\u001b[A\n",
            "Iteration:  43% 170/392 [01:51<02:37,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 171/392 [01:51<02:36,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 172/392 [01:52<02:36,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 173/392 [01:53<02:35,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 174/392 [01:53<02:35,  1.41it/s]\u001b[A\n",
            "Iteration:  45% 175/392 [01:54<02:34,  1.40it/s]\u001b[A\n",
            "Iteration:  45% 176/392 [01:55<02:34,  1.40it/s]\u001b[A\n",
            "Iteration:  45% 177/392 [01:56<02:33,  1.40it/s]\u001b[A\n",
            "Iteration:  45% 178/392 [01:56<02:33,  1.39it/s]\u001b[A\n",
            "Iteration:  46% 179/392 [01:57<02:33,  1.39it/s]\u001b[A\n",
            "Iteration:  46% 180/392 [01:58<02:32,  1.39it/s]\u001b[A\n",
            "Iteration:  46% 181/392 [01:59<02:32,  1.38it/s]\u001b[A\n",
            "Iteration:  46% 182/392 [01:59<02:32,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 183/392 [02:00<02:31,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 184/392 [02:01<02:31,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 185/392 [02:01<02:30,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 186/392 [02:02<02:30,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 187/392 [02:03<02:29,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 188/392 [02:04<02:28,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 189/392 [02:04<02:28,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 190/392 [02:05<02:28,  1.36it/s]\u001b[A\n",
            "Iteration:  49% 191/392 [02:06<02:27,  1.36it/s]\u001b[A\n",
            "Iteration:  49% 192/392 [02:07<02:26,  1.36it/s]\u001b[A\n",
            "Iteration:  49% 193/392 [02:07<02:25,  1.36it/s]\u001b[A\n",
            "Iteration:  49% 194/392 [02:08<02:25,  1.36it/s]\u001b[A\n",
            "Iteration:  50% 195/392 [02:09<02:23,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 196/392 [02:09<02:23,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 197/392 [02:10<02:22,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 198/392 [02:11<02:21,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 199/392 [02:12<02:20,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 200/392 [02:12<02:20,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 201/392 [02:13<02:18,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 202/392 [02:14<02:18,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 203/392 [02:15<02:16,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 204/392 [02:15<02:15,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 205/392 [02:16<02:14,  1.39it/s]\u001b[A\n",
            "Iteration:  53% 206/392 [02:17<02:13,  1.39it/s]\u001b[A\n",
            "Iteration:  53% 207/392 [02:17<02:12,  1.39it/s]\u001b[A\n",
            "Iteration:  53% 208/392 [02:18<02:11,  1.40it/s]\u001b[A\n",
            "Iteration:  53% 209/392 [02:19<02:10,  1.40it/s]\u001b[A\n",
            "Iteration:  54% 210/392 [02:20<02:09,  1.40it/s]\u001b[A\n",
            "Iteration:  54% 211/392 [02:20<02:08,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 212/392 [02:21<02:08,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 213/392 [02:22<02:07,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 214/392 [02:22<02:06,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 215/392 [02:23<02:05,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 216/392 [02:24<02:04,  1.41it/s]\u001b[A\n",
            "Iteration:  55% 217/392 [02:25<02:03,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 218/392 [02:25<02:03,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 219/392 [02:26<02:02,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 220/392 [02:27<02:01,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 221/392 [02:27<02:01,  1.41it/s]\u001b[A\n",
            "Iteration:  57% 222/392 [02:28<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 223/392 [02:29<01:59,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 224/392 [02:29<01:58,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 225/392 [02:30<01:57,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 226/392 [02:31<01:56,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 227/392 [02:32<01:55,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 228/392 [02:32<01:54,  1.43it/s]\u001b[A\n",
            "Iteration:  58% 229/392 [02:33<01:54,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 230/392 [02:34<01:53,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 231/392 [02:34<01:52,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 232/392 [02:35<01:52,  1.43it/s]\u001b[A\n",
            "Iteration:  59% 233/392 [02:36<01:51,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 234/392 [02:36<01:50,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 235/392 [02:37<01:49,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 236/392 [02:38<01:48,  1.43it/s]\u001b[A\n",
            "Iteration:  60% 237/392 [02:39<01:48,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 238/392 [02:39<01:47,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 239/392 [02:40<01:46,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 240/392 [02:41<01:46,  1.43it/s]\u001b[A\n",
            "Iteration:  61% 241/392 [02:41<01:45,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 242/392 [02:42<01:44,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 243/392 [02:43<01:44,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 244/392 [02:43<01:43,  1.43it/s]\u001b[A\n",
            "Iteration:  62% 245/392 [02:44<01:42,  1.43it/s]\u001b[A\n",
            "Iteration:  63% 246/392 [02:45<01:41,  1.43it/s]\u001b[A\n",
            "Iteration:  63% 247/392 [02:46<01:41,  1.43it/s]\u001b[A\n",
            "Iteration:  63% 248/392 [02:46<01:40,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 249/392 [02:47<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 250/392 [02:48<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 251/392 [02:48<01:38,  1.43it/s]\u001b[A\n",
            "Iteration:  64% 252/392 [02:49<01:37,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 253/392 [02:50<01:37,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 254/392 [02:50<01:36,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 255/392 [02:51<01:35,  1.43it/s]\u001b[A\n",
            "Iteration:  65% 256/392 [02:52<01:34,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 257/392 [02:53<01:34,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 258/392 [02:53<01:33,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 259/392 [02:54<01:33,  1.43it/s]\u001b[A\n",
            "Iteration:  66% 260/392 [02:55<01:32,  1.43it/s]\u001b[A\n",
            "Iteration:  67% 261/392 [02:55<01:31,  1.43it/s]\u001b[A\n",
            "Iteration:  67% 262/392 [02:56<01:31,  1.43it/s]\u001b[A\n",
            "Iteration:  67% 263/392 [02:57<01:30,  1.42it/s]\u001b[A\n",
            "Iteration:  67% 264/392 [02:57<01:29,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 265/392 [02:58<01:29,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 266/392 [02:59<01:28,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 267/392 [03:00<01:27,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 268/392 [03:00<01:27,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 269/392 [03:01<01:26,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 270/392 [03:02<01:26,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 271/392 [03:02<01:25,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 272/392 [03:03<01:24,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 273/392 [03:04<01:24,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 274/392 [03:05<01:23,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 275/392 [03:05<01:22,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 276/392 [03:06<01:22,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 277/392 [03:07<01:21,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 278/392 [03:07<01:20,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 279/392 [03:08<01:20,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 280/392 [03:09<01:19,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 281/392 [03:09<01:18,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 282/392 [03:10<01:18,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 283/392 [03:11<01:17,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 284/392 [03:12<01:16,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 285/392 [03:12<01:15,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 286/392 [03:13<01:15,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 287/392 [03:14<01:14,  1.41it/s]\u001b[A\n",
            "Iteration:  73% 288/392 [03:14<01:13,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 289/392 [03:15<01:13,  1.40it/s]\u001b[A\n",
            "Iteration:  74% 290/392 [03:16<01:12,  1.40it/s]\u001b[A\n",
            "Iteration:  74% 291/392 [03:17<01:11,  1.40it/s]\u001b[A\n",
            "Iteration:  74% 292/392 [03:17<01:11,  1.40it/s]\u001b[A\n",
            "Iteration:  75% 293/392 [03:18<01:10,  1.41it/s]\u001b[A\n",
            "Iteration:  75% 294/392 [03:19<01:09,  1.40it/s]\u001b[A\n",
            "Iteration:  75% 295/392 [03:19<01:09,  1.40it/s]\u001b[A\n",
            "Iteration:  76% 296/392 [03:20<01:08,  1.40it/s]\u001b[A\n",
            "Iteration:  76% 297/392 [03:21<01:07,  1.40it/s]\u001b[A\n",
            "Iteration:  76% 298/392 [03:22<01:07,  1.40it/s]\u001b[A\n",
            "Iteration:  76% 299/392 [03:22<01:06,  1.40it/s]\u001b[A\n",
            "Iteration:  77% 300/392 [03:23<01:05,  1.40it/s]\u001b[A\n",
            "Iteration:  77% 301/392 [03:24<01:04,  1.40it/s]\u001b[A\n",
            "Iteration:  77% 302/392 [03:24<01:04,  1.40it/s]\u001b[A\n",
            "Iteration:  77% 303/392 [03:25<01:03,  1.40it/s]\u001b[A\n",
            "Iteration:  78% 304/392 [03:26<01:02,  1.40it/s]\u001b[A\n",
            "Iteration:  78% 305/392 [03:27<01:01,  1.40it/s]\u001b[A\n",
            "Iteration:  78% 306/392 [03:27<01:01,  1.41it/s]\u001b[A\n",
            "Iteration:  78% 307/392 [03:28<01:00,  1.40it/s]\u001b[A\n",
            "Iteration:  79% 308/392 [03:29<00:59,  1.40it/s]\u001b[A\n",
            "Iteration:  79% 309/392 [03:29<00:59,  1.40it/s]\u001b[A\n",
            "Iteration:  79% 310/392 [03:30<00:58,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 311/392 [03:31<00:57,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 312/392 [03:32<00:56,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 313/392 [03:32<00:56,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 314/392 [03:33<00:55,  1.41it/s]\u001b[A\n",
            "Iteration:  80% 315/392 [03:34<00:54,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 316/392 [03:34<00:53,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 317/392 [03:35<00:53,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 318/392 [03:36<00:52,  1.41it/s]\u001b[A\n",
            "Iteration:  81% 319/392 [03:37<00:51,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 320/392 [03:37<00:51,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 321/392 [03:38<00:50,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 322/392 [03:39<00:49,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 323/392 [03:39<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 324/392 [03:40<00:48,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 325/392 [03:41<00:47,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 326/392 [03:41<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 327/392 [03:42<00:46,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 328/392 [03:43<00:45,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 329/392 [03:44<00:44,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 330/392 [03:44<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  84% 331/392 [03:45<00:43,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 332/392 [03:46<00:42,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 333/392 [03:46<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 334/392 [03:47<00:41,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 335/392 [03:48<00:40,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 336/392 [03:49<00:39,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 337/392 [03:49<00:38,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 338/392 [03:50<00:38,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 339/392 [03:51<00:37,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 340/392 [03:51<00:36,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 341/392 [03:52<00:36,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 342/392 [03:53<00:35,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 343/392 [03:54<00:34,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 344/392 [03:54<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 345/392 [03:55<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 346/392 [03:56<00:32,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 347/392 [03:56<00:31,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 348/392 [03:57<00:31,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 349/392 [03:58<00:30,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 350/392 [03:58<00:29,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 351/392 [03:59<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 352/392 [04:00<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 353/392 [04:01<00:27,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 354/392 [04:01<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 355/392 [04:02<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 356/392 [04:03<00:25,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 357/392 [04:03<00:24,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 358/392 [04:04<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 359/392 [04:05<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 360/392 [04:06<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 361/392 [04:06<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 362/392 [04:07<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 363/392 [04:08<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 364/392 [04:08<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 365/392 [04:09<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 366/392 [04:10<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 367/392 [04:10<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 368/392 [04:11<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 369/392 [04:12<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 370/392 [04:13<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 371/392 [04:13<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 372/392 [04:14<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 373/392 [04:15<00:13,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 374/392 [04:15<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  96% 375/392 [04:16<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  96% 376/392 [04:17<00:11,  1.41it/s]\u001b[A\n",
            "Iteration:  96% 377/392 [04:18<00:10,  1.41it/s]\u001b[A\n",
            "Iteration:  96% 378/392 [04:18<00:09,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 379/392 [04:19<00:09,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 380/392 [04:20<00:08,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 381/392 [04:20<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 382/392 [04:21<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 383/392 [04:22<00:06,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 384/392 [04:22<00:05,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 385/392 [04:23<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 386/392 [04:24<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 387/392 [04:25<00:03,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 388/392 [04:25<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 389/392 [04:26<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 390/392 [04:27<00:01,  1.41it/s]\u001b[A\n",
            "Iteration: 100% 391/392 [04:27<00:00,  1.41it/s]\u001b[A\n",
            "Iteration: 100% 392/392 [04:28<00:00,  1.46it/s]\n",
            "12/02/2020 17:43:35 - INFO - __main__ - ***** Average training loss: 0.48 *****\n",
            "12/02/2020 17:43:35 - INFO - __main__ - ***** Training epoch took: 0:04:29 *****\n",
            "Epoch: 100% 1/1 [04:28<00:00, 268.62s/it]\n",
            "12/02/2020 17:43:35 - INFO - __main__ - Creating features from dataset file at data\n",
            "12/02/2020 17:43:36 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:43:36 - INFO - processors - guid: dev-0\n",
            "12/02/2020 17:43:36 - INFO - processors - tokens: [CLS] from the ap comes this story : [SEP]\n",
            "12/02/2020 17:43:36 - INFO - processors - input_ids: 101 2013 1996 9706 3310 2023 2466 1024 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - label_ids: -100 1 5 11 15 5 7 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:43:36 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:43:36 - INFO - processors - guid: dev-1\n",
            "12/02/2020 17:43:36 - INFO - processors - tokens: [CLS] president bush on tuesday nominated two individuals to replace retiring jurist ##s on federal courts in the washington area . [SEP]\n",
            "12/02/2020 17:43:36 - INFO - processors - input_ids: 101 2343 5747 2006 9857 4222 2048 3633 2000 5672 9150 22757 2015 2006 2976 5434 1999 1996 2899 2181 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - label_ids: -100 11 11 1 11 15 8 7 9 15 15 7 -100 1 0 7 1 5 11 7 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:43:36 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:43:36 - INFO - processors - guid: dev-2\n",
            "12/02/2020 17:43:36 - INFO - processors - tokens: [CLS] bush nominated jennifer m . anderson for a 15 - year term as associate judge of the superior court of the district of columbia , replacing ste ##ffen w . gr ##aa ##e . [SEP]\n",
            "12/02/2020 17:43:36 - INFO - processors - input_ids: 101 5747 4222 7673 1049 1012 5143 2005 1037 2321 1011 2095 2744 2004 5482 3648 1997 1996 6020 2457 1997 1996 2212 1997 3996 1010 6419 26261 18032 1059 1012 24665 11057 2063 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - label_ids: -100 11 15 11 11 -100 11 1 5 8 12 7 7 1 0 7 1 5 11 11 1 5 11 1 11 12 15 11 -100 11 -100 11 -100 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:43:36 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:43:36 - INFO - processors - guid: dev-3\n",
            "12/02/2020 17:43:36 - INFO - processors - tokens: [CLS] * * * [SEP]\n",
            "12/02/2020 17:43:36 - INFO - processors - input_ids: 101 1008 1008 1008 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - label_ids: -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:43:36 - INFO - processors - *** Example ***\n",
            "12/02/2020 17:43:36 - INFO - processors - guid: dev-4\n",
            "12/02/2020 17:43:36 - INFO - processors - tokens: [CLS] bush also nominated a . noel an ##ke ##tell kramer for a 15 - year term as associate judge of the district of columbia court of appeals , replacing john montagu ##e ste ##ad ##man . [SEP]\n",
            "12/02/2020 17:43:36 - INFO - processors - input_ids: 101 5747 2036 4222 1037 1012 10716 2019 3489 23567 16322 2005 1037 2321 1011 2095 2744 2004 5482 3648 1997 1996 2212 1997 3996 2457 1997 9023 1010 6419 2198 26241 2063 26261 4215 2386 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 17:43:36 - INFO - processors - label_ids: -100 11 2 15 11 -100 11 11 -100 -100 11 1 5 8 12 7 7 1 0 7 1 5 11 1 11 11 1 11 12 15 11 11 -100 11 -100 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "12/02/2020 17:43:37 - INFO - __main__ - ***** Running evaluation:: Task : pos, Prefix : Current Task *****\n",
            "12/02/2020 17:43:37 - INFO - __main__ -   Num examples = 2002\n",
            "12/02/2020 17:43:37 - INFO - __main__ -   Batch size = 32\n",
            "Evaluating: 100% 63/63 [00:16<00:00,  3.92it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: _ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "12/02/2020 17:43:54 - INFO - __main__ - ***** Eval results Current Task pos*****\n",
            "12/02/2020 17:43:54 - INFO - __main__ -  acc = 0.9591019703273412\n",
            "12/02/2020 17:43:54 - INFO - __main__ -  f1 = 0.9518212375536435\n",
            "12/02/2020 17:43:54 - INFO - __main__ -  loss = 0.14873617461749486\n",
            "12/02/2020 17:43:54 - INFO - __main__ -  precision = 0.9513586612010312\n",
            "12/02/2020 17:43:54 - INFO - __main__ -  recall = 0.9522842639593908\n",
            "12/02/2020 17:43:55 - INFO - __main__ -  global_step = 392, average loss = 0.4780869002130871\n",
            "\n",
            "***** Accuracy Matrix *****\n",
            "\n",
            "[[0.959102]]\n",
            "\n",
            "***** Transfer Matrix *****\n",
            "Future Transfer => Upper Triangular Matrix  ||  Backward Transfer => Lower Triangular Matrix\n",
            "\n",
            "[[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuLhZMqPT2jk",
        "outputId": "06132161-aca0-4669-94e9-b5f585196068"
      },
      "source": [
        "!python main.py \\\n",
        "  --data_dir \"data\" \\\n",
        "  --task_params \"example_task.json\" \\\n",
        "  --cuda \\\n",
        "  --tokenizer_name \"bert\" \\\n",
        "  --do_lower_case \\\n",
        "  --model_type \"bert-base-uncased\" \\\n",
        "  --output_dir \"out\" \\\n",
        "  --per_gpu_batch_size 32 \\\n",
        "  --num_train_epochs 2 \\\n",
        "  --seed 42 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 22:52:07.293859: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Namespace(cuda=True, data_dir='data', device=device(type='cuda'), do_lower_case=True, max_seq_length=128, model_type='bert-base-uncased', n_gpu=1, num_train_epochs=2, output_dir='out', per_gpu_batch_size=32, plot=False, seed=42, task_params={'cola': {'learning_rate': 3e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}, 'mrpc': {'learning_rate': 3e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}, 'rte': {'learning_rate': 3e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'logging_steps': 500, 'save_steps': 500, 'output_mode': 'classification'}}, tokenizer_name='bert', warmup_steps=0)\n",
            "{}\n",
            "{'cola': <transformers.data.processors.glue.ColaProcessor object at 0x7f91df7de828>}\n",
            "{'cola': <transformers.data.processors.glue.ColaProcessor object at 0x7f91df7de828>, 'mrpc': <transformers.data.processors.glue.MrpcProcessor object at 0x7f91df7de710>}\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:52:09 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:52:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "12/02/2020 22:52:10 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": null,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/02/2020 22:52:10 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "12/02/2020 22:52:10 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:52:14 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:52:14 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 22:52:14 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:52:18 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:52:18 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "12/02/2020 22:52:19 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "12/02/2020 22:52:22 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "12/02/2020 22:52:22 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "\n",
            "***** Parameters Saved for task 0 *****\n",
            "\n",
            "\n",
            "\n",
            "***** Parameters Saved for task 1 *****\n",
            "\n",
            "\n",
            "\n",
            "***** Parameters Saved for task 2 *****\n",
            "\n",
            "\n",
            "12/02/2020 22:52:33 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   Writing example 0/8551\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2256 2814 2180 1005 1056 4965 2023 4106 1010 2292 2894 1996 2279 2028 2057 16599 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2028 2062 18404 2236 3989 1998 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2028 2062 18404 2236 3989 2030 1045 1005 1049 3228 2039 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 2062 2057 2817 16025 1010 1996 13675 16103 2121 2027 2131 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   input_ids: 101 2154 2011 2154 1996 8866 2024 2893 14163 8024 3771 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:52:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:52:35 - INFO - __main__ -   ***** Running training *****\n",
            "12/02/2020 22:52:35 - INFO - __main__ -     Num examples = 8551\n",
            "12/02/2020 22:52:35 - INFO - __main__ -     Num Epochs = 2\n",
            "12/02/2020 22:52:35 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "12/02/2020 22:52:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "12/02/2020 22:52:35 - INFO - __main__ -     Total optimization steps = 536\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 1/268 [00:00<03:11,  1.39it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:01<03:06,  1.43it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:02<03:03,  1.45it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:02<03:00,  1.46it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:03<02:58,  1.47it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:04<02:57,  1.48it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:04<02:56,  1.48it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:05<02:55,  1.48it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:06<02:53,  1.49it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:06<02:53,  1.49it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:07<02:52,  1.49it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:08<02:52,  1.48it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:08<02:51,  1.48it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:09<02:51,  1.48it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:10<02:51,  1.48it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:10<02:51,  1.47it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:11<02:50,  1.47it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:12<02:49,  1.47it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:12<02:49,  1.47it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:13<02:48,  1.47it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:14<02:48,  1.47it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:14<02:47,  1.47it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:15<02:47,  1.47it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:16<02:46,  1.47it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:16<02:46,  1.46it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:17<02:45,  1.46it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:18<02:45,  1.46it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:19<02:44,  1.46it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:19<02:44,  1.46it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:20<02:43,  1.46it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:21<02:43,  1.45it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:21<02:42,  1.45it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:22<02:41,  1.45it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:23<02:41,  1.45it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:23<02:40,  1.45it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:24<02:40,  1.45it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:25<02:39,  1.44it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:25<02:39,  1.44it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:26<02:39,  1.44it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:27<02:38,  1.43it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:28<02:38,  1.43it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:28<02:37,  1.43it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:29<02:37,  1.43it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:30<02:36,  1.43it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:30<02:36,  1.43it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:31<02:35,  1.43it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:32<02:35,  1.42it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:32<02:34,  1.42it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:33<02:34,  1.42it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:34<02:34,  1.41it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:35<02:33,  1.41it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:35<02:33,  1.41it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:36<02:32,  1.41it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:37<02:32,  1.41it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:37<02:31,  1.41it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:38<02:30,  1.41it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:39<02:30,  1.41it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:40<02:29,  1.41it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:40<02:29,  1.40it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:41<02:28,  1.40it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:42<02:28,  1.39it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:42<02:27,  1.39it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:43<02:27,  1.39it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:44<02:26,  1.39it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:45<02:26,  1.39it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:45<02:25,  1.39it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:46<02:24,  1.39it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:47<02:24,  1.38it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:47<02:23,  1.38it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:48<02:23,  1.38it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:49<02:22,  1.38it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:50<02:22,  1.38it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:50<02:21,  1.38it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:51<02:20,  1.38it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:52<02:19,  1.38it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:53<02:19,  1.38it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:53<02:18,  1.38it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:54<02:17,  1.39it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:55<02:16,  1.38it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:55<02:15,  1.39it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:56<02:14,  1.39it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:57<02:13,  1.39it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:58<02:12,  1.40it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:58<02:11,  1.40it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:59<02:10,  1.40it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [01:00<02:09,  1.40it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [01:00<02:09,  1.40it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [01:01<02:08,  1.41it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [01:02<02:07,  1.41it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [01:03<02:06,  1.41it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [01:03<02:05,  1.41it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [01:04<02:04,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [01:05<02:03,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [01:05<02:02,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [01:06<02:01,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [01:07<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [01:07<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [01:08<01:59,  1.43it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [01:09<01:58,  1.43it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [01:10<01:57,  1.43it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [01:10<01:56,  1.43it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [01:11<01:55,  1.43it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [01:12<01:54,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [01:12<01:54,  1.44it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [01:13<01:53,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [01:14<01:52,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [01:14<01:51,  1.44it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [01:15<01:50,  1.44it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [01:16<01:50,  1.44it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [01:17<01:49,  1.44it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [01:17<01:48,  1.44it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [01:18<01:48,  1.44it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [01:19<01:47,  1.44it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [01:19<01:46,  1.44it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [01:20<01:46,  1.44it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [01:21<01:45,  1.44it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [01:21<01:44,  1.45it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [01:22<01:43,  1.45it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [01:23<01:42,  1.45it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [01:23<01:42,  1.45it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [01:24<01:41,  1.45it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [01:25<01:41,  1.45it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [01:25<01:40,  1.45it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [01:26<01:39,  1.45it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [01:27<01:38,  1.45it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [01:28<01:37,  1.45it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [01:28<01:37,  1.45it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [01:29<01:36,  1.45it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [01:30<01:36,  1.44it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [01:30<01:35,  1.44it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [01:31<01:34,  1.45it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [01:32<01:34,  1.44it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [01:32<01:33,  1.45it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [01:33<01:32,  1.45it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [01:34<01:32,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [01:34<01:31,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [01:35<01:30,  1.44it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [01:36<01:30,  1.44it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [01:37<01:29,  1.44it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [01:37<01:29,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [01:38<01:28,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [01:39<01:27,  1.44it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [01:39<01:26,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [01:40<01:26,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [01:41<01:25,  1.44it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [01:41<01:25,  1.43it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [01:42<01:24,  1.43it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [01:43<01:23,  1.43it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [01:44<01:23,  1.43it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [01:44<01:22,  1.43it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [01:45<01:22,  1.43it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [01:46<01:21,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [01:46<01:20,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [01:47<01:20,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [01:48<01:19,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [01:48<01:19,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:49<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:50<01:17,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:51<01:17,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:51<01:16,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:52<01:15,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:53<01:15,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:53<01:14,  1.41it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:54<01:14,  1.40it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:55<01:13,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:56<01:12,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:56<01:11,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:57<01:10,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:58<01:10,  1.41it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:58<01:09,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:59<01:08,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [02:00<01:08,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [02:01<01:07,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [02:01<01:06,  1.41it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [02:02<01:05,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [02:03<01:05,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [02:03<01:04,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [02:04<01:03,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [02:05<01:03,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [02:06<01:02,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [02:06<01:01,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [02:07<01:00,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [02:08<01:00,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [02:08<00:59,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [02:09<00:58,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [02:10<00:57,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [02:10<00:57,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [02:11<00:56,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [02:12<00:55,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [02:13<00:54,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [02:13<00:54,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [02:14<00:53,  1.43it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [02:15<00:52,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [02:15<00:51,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [02:16<00:51,  1.43it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [02:17<00:50,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [02:17<00:49,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [02:18<00:49,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [02:19<00:48,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [02:20<00:47,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [02:20<00:47,  1.42it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [02:21<00:46,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [02:22<00:45,  1.43it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [02:22<00:44,  1.43it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [02:23<00:43,  1.43it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [02:24<00:43,  1.43it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [02:24<00:42,  1.43it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [02:25<00:41,  1.44it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [02:26<00:41,  1.43it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [02:27<00:40,  1.43it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [02:27<00:39,  1.43it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [02:28<00:39,  1.44it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [02:29<00:38,  1.44it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [02:29<00:37,  1.44it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [02:30<00:36,  1.44it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [02:31<00:36,  1.43it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [02:31<00:35,  1.44it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [02:32<00:34,  1.44it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [02:33<00:34,  1.44it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [02:34<00:33,  1.44it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [02:34<00:32,  1.44it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [02:35<00:32,  1.43it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [02:36<00:31,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [02:36<00:30,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [02:37<00:30,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [02:38<00:29,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [02:38<00:28,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [02:39<00:27,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [02:40<00:27,  1.43it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [02:41<00:26,  1.43it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [02:41<00:25,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 232/268 [02:42<00:25,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 233/268 [02:43<00:24,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 234/268 [02:43<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 235/268 [02:44<00:23,  1.43it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [02:45<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [02:45<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [02:46<00:21,  1.43it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [02:47<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [02:48<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [02:48<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [02:49<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [02:50<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [02:50<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [02:51<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [02:52<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [02:52<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [02:53<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [02:54<00:13,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [02:55<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [02:55<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [02:56<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [02:57<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [02:57<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [02:58<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [02:59<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [03:00<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [03:00<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [03:01<00:06,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [03:02<00:05,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [03:02<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [03:03<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [03:04<00:03,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [03:04<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [03:05<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [03:06<00:01,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [03:07<00:00,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [03:07<00:00,  1.43it/s]\n",
            "Epoch:  50% 1/2 [03:07<03:07, 187.30s/it]\n",
            "Iteration:   0% 0/268 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/268 [00:00<03:05,  1.44it/s]\u001b[A\n",
            "Iteration:   1% 2/268 [00:01<03:05,  1.43it/s]\u001b[A\n",
            "Iteration:   1% 3/268 [00:02<03:05,  1.43it/s]\u001b[A\n",
            "Iteration:   1% 4/268 [00:02<03:05,  1.43it/s]\u001b[A\n",
            "Iteration:   2% 5/268 [00:03<03:04,  1.42it/s]\u001b[A\n",
            "Iteration:   2% 6/268 [00:04<03:03,  1.42it/s]\u001b[A\n",
            "Iteration:   3% 7/268 [00:04<03:03,  1.42it/s]\u001b[A\n",
            "Iteration:   3% 8/268 [00:05<03:02,  1.42it/s]\u001b[A\n",
            "Iteration:   3% 9/268 [00:06<03:01,  1.43it/s]\u001b[A\n",
            "Iteration:   4% 10/268 [00:07<03:01,  1.42it/s]\u001b[A\n",
            "Iteration:   4% 11/268 [00:07<03:00,  1.42it/s]\u001b[A\n",
            "Iteration:   4% 12/268 [00:08<02:59,  1.42it/s]\u001b[A\n",
            "Iteration:   5% 13/268 [00:09<02:59,  1.42it/s]\u001b[A\n",
            "Iteration:   5% 14/268 [00:09<02:58,  1.42it/s]\u001b[A\n",
            "Iteration:   6% 15/268 [00:10<02:57,  1.43it/s]\u001b[A\n",
            "Iteration:   6% 16/268 [00:11<02:56,  1.42it/s]\u001b[A\n",
            "Iteration:   6% 17/268 [00:11<02:56,  1.43it/s]\u001b[A\n",
            "Iteration:   7% 18/268 [00:12<02:55,  1.43it/s]\u001b[A\n",
            "Iteration:   7% 19/268 [00:13<02:54,  1.43it/s]\u001b[A\n",
            "Iteration:   7% 20/268 [00:14<02:53,  1.43it/s]\u001b[A\n",
            "Iteration:   8% 21/268 [00:14<02:52,  1.43it/s]\u001b[A\n",
            "Iteration:   8% 22/268 [00:15<02:51,  1.43it/s]\u001b[A\n",
            "Iteration:   9% 23/268 [00:16<02:50,  1.43it/s]\u001b[A\n",
            "Iteration:   9% 24/268 [00:16<02:50,  1.43it/s]\u001b[A\n",
            "Iteration:   9% 25/268 [00:17<02:49,  1.43it/s]\u001b[A\n",
            "Iteration:  10% 26/268 [00:18<02:49,  1.43it/s]\u001b[A\n",
            "Iteration:  10% 27/268 [00:18<02:47,  1.43it/s]\u001b[A\n",
            "Iteration:  10% 28/268 [00:19<02:47,  1.43it/s]\u001b[A\n",
            "Iteration:  11% 29/268 [00:20<02:46,  1.43it/s]\u001b[A\n",
            "Iteration:  11% 30/268 [00:21<02:45,  1.44it/s]\u001b[A\n",
            "Iteration:  12% 31/268 [00:21<02:45,  1.44it/s]\u001b[A\n",
            "Iteration:  12% 32/268 [00:22<02:44,  1.44it/s]\u001b[A\n",
            "Iteration:  12% 33/268 [00:23<02:43,  1.43it/s]\u001b[A\n",
            "Iteration:  13% 34/268 [00:23<02:42,  1.44it/s]\u001b[A\n",
            "Iteration:  13% 35/268 [00:24<02:42,  1.44it/s]\u001b[A\n",
            "Iteration:  13% 36/268 [00:25<02:41,  1.44it/s]\u001b[A\n",
            "Iteration:  14% 37/268 [00:25<02:40,  1.44it/s]\u001b[A\n",
            "Iteration:  14% 38/268 [00:26<02:40,  1.44it/s]\u001b[A\n",
            "Iteration:  15% 39/268 [00:27<02:39,  1.44it/s]\u001b[A\n",
            "Iteration:  15% 40/268 [00:27<02:39,  1.43it/s]\u001b[A\n",
            "Iteration:  15% 41/268 [00:28<02:38,  1.44it/s]\u001b[A\n",
            "Iteration:  16% 42/268 [00:29<02:37,  1.43it/s]\u001b[A\n",
            "Iteration:  16% 43/268 [00:30<02:36,  1.43it/s]\u001b[A\n",
            "Iteration:  16% 44/268 [00:30<02:36,  1.43it/s]\u001b[A\n",
            "Iteration:  17% 45/268 [00:31<02:35,  1.43it/s]\u001b[A\n",
            "Iteration:  17% 46/268 [00:32<02:34,  1.43it/s]\u001b[A\n",
            "Iteration:  18% 47/268 [00:32<02:34,  1.43it/s]\u001b[A\n",
            "Iteration:  18% 48/268 [00:33<02:33,  1.43it/s]\u001b[A\n",
            "Iteration:  18% 49/268 [00:34<02:33,  1.43it/s]\u001b[A\n",
            "Iteration:  19% 50/268 [00:34<02:32,  1.43it/s]\u001b[A\n",
            "Iteration:  19% 51/268 [00:35<02:31,  1.43it/s]\u001b[A\n",
            "Iteration:  19% 52/268 [00:36<02:30,  1.43it/s]\u001b[A\n",
            "Iteration:  20% 53/268 [00:37<02:30,  1.43it/s]\u001b[A\n",
            "Iteration:  20% 54/268 [00:37<02:29,  1.43it/s]\u001b[A\n",
            "Iteration:  21% 55/268 [00:38<02:28,  1.43it/s]\u001b[A\n",
            "Iteration:  21% 56/268 [00:39<02:28,  1.43it/s]\u001b[A\n",
            "Iteration:  21% 57/268 [00:39<02:28,  1.42it/s]\u001b[A\n",
            "Iteration:  22% 58/268 [00:40<02:27,  1.43it/s]\u001b[A\n",
            "Iteration:  22% 59/268 [00:41<02:26,  1.42it/s]\u001b[A\n",
            "Iteration:  22% 60/268 [00:41<02:25,  1.43it/s]\u001b[A\n",
            "Iteration:  23% 61/268 [00:42<02:25,  1.43it/s]\u001b[A\n",
            "Iteration:  23% 62/268 [00:43<02:24,  1.42it/s]\u001b[A\n",
            "Iteration:  24% 63/268 [00:44<02:23,  1.42it/s]\u001b[A\n",
            "Iteration:  24% 64/268 [00:44<02:23,  1.43it/s]\u001b[A\n",
            "Iteration:  24% 65/268 [00:45<02:22,  1.42it/s]\u001b[A\n",
            "Iteration:  25% 66/268 [00:46<02:22,  1.42it/s]\u001b[A\n",
            "Iteration:  25% 67/268 [00:46<02:21,  1.42it/s]\u001b[A\n",
            "Iteration:  25% 68/268 [00:47<02:20,  1.42it/s]\u001b[A\n",
            "Iteration:  26% 69/268 [00:48<02:20,  1.42it/s]\u001b[A\n",
            "Iteration:  26% 70/268 [00:49<02:19,  1.42it/s]\u001b[A\n",
            "Iteration:  26% 71/268 [00:49<02:19,  1.42it/s]\u001b[A\n",
            "Iteration:  27% 72/268 [00:50<02:18,  1.41it/s]\u001b[A\n",
            "Iteration:  27% 73/268 [00:51<02:17,  1.41it/s]\u001b[A\n",
            "Iteration:  28% 74/268 [00:51<02:17,  1.41it/s]\u001b[A\n",
            "Iteration:  28% 75/268 [00:52<02:16,  1.41it/s]\u001b[A\n",
            "Iteration:  28% 76/268 [00:53<02:15,  1.42it/s]\u001b[A\n",
            "Iteration:  29% 77/268 [00:53<02:14,  1.42it/s]\u001b[A\n",
            "Iteration:  29% 78/268 [00:54<02:14,  1.42it/s]\u001b[A\n",
            "Iteration:  29% 79/268 [00:55<02:13,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 80/268 [00:56<02:12,  1.42it/s]\u001b[A\n",
            "Iteration:  30% 81/268 [00:56<02:12,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 82/268 [00:57<02:11,  1.42it/s]\u001b[A\n",
            "Iteration:  31% 83/268 [00:58<02:10,  1.42it/s]\u001b[A\n",
            "Iteration:  31% 84/268 [00:58<02:09,  1.42it/s]\u001b[A\n",
            "Iteration:  32% 85/268 [00:59<02:09,  1.41it/s]\u001b[A\n",
            "Iteration:  32% 86/268 [01:00<02:08,  1.42it/s]\u001b[A\n",
            "Iteration:  32% 87/268 [01:01<02:07,  1.42it/s]\u001b[A\n",
            "Iteration:  33% 88/268 [01:01<02:07,  1.42it/s]\u001b[A\n",
            "Iteration:  33% 89/268 [01:02<02:06,  1.42it/s]\u001b[A\n",
            "Iteration:  34% 90/268 [01:03<02:05,  1.42it/s]\u001b[A\n",
            "Iteration:  34% 91/268 [01:03<02:04,  1.42it/s]\u001b[A\n",
            "Iteration:  34% 92/268 [01:04<02:04,  1.41it/s]\u001b[A\n",
            "Iteration:  35% 93/268 [01:05<02:03,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 94/268 [01:05<02:02,  1.42it/s]\u001b[A\n",
            "Iteration:  35% 95/268 [01:06<02:01,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 96/268 [01:07<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 97/268 [01:08<02:00,  1.42it/s]\u001b[A\n",
            "Iteration:  37% 98/268 [01:08<01:59,  1.42it/s]\u001b[A\n",
            "Iteration:  37% 99/268 [01:09<01:58,  1.42it/s]\u001b[A\n",
            "Iteration:  37% 100/268 [01:10<01:58,  1.42it/s]\u001b[A\n",
            "Iteration:  38% 101/268 [01:10<01:57,  1.42it/s]\u001b[A\n",
            "Iteration:  38% 102/268 [01:11<01:56,  1.42it/s]\u001b[A\n",
            "Iteration:  38% 103/268 [01:12<01:55,  1.43it/s]\u001b[A\n",
            "Iteration:  39% 104/268 [01:12<01:55,  1.43it/s]\u001b[A\n",
            "Iteration:  39% 105/268 [01:13<01:54,  1.42it/s]\u001b[A\n",
            "Iteration:  40% 106/268 [01:14<01:53,  1.43it/s]\u001b[A\n",
            "Iteration:  40% 107/268 [01:15<01:52,  1.43it/s]\u001b[A\n",
            "Iteration:  40% 108/268 [01:15<01:52,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 109/268 [01:16<01:51,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 110/268 [01:17<01:50,  1.43it/s]\u001b[A\n",
            "Iteration:  41% 111/268 [01:17<01:49,  1.43it/s]\u001b[A\n",
            "Iteration:  42% 112/268 [01:18<01:49,  1.43it/s]\u001b[A\n",
            "Iteration:  42% 113/268 [01:19<01:48,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 114/268 [01:19<01:48,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 115/268 [01:20<01:47,  1.43it/s]\u001b[A\n",
            "Iteration:  43% 116/268 [01:21<01:46,  1.43it/s]\u001b[A\n",
            "Iteration:  44% 117/268 [01:22<01:45,  1.43it/s]\u001b[A\n",
            "Iteration:  44% 118/268 [01:22<01:44,  1.43it/s]\u001b[A\n",
            "Iteration:  44% 119/268 [01:23<01:44,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 120/268 [01:24<01:43,  1.43it/s]\u001b[A\n",
            "Iteration:  45% 121/268 [01:24<01:42,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 122/268 [01:25<01:42,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 123/268 [01:26<01:41,  1.43it/s]\u001b[A\n",
            "Iteration:  46% 124/268 [01:26<01:40,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 125/268 [01:27<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 126/268 [01:28<01:39,  1.43it/s]\u001b[A\n",
            "Iteration:  47% 127/268 [01:29<01:38,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 128/268 [01:29<01:37,  1.43it/s]\u001b[A\n",
            "Iteration:  48% 129/268 [01:30<01:37,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 130/268 [01:31<01:36,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 131/268 [01:31<01:35,  1.43it/s]\u001b[A\n",
            "Iteration:  49% 132/268 [01:32<01:35,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 133/268 [01:33<01:34,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 134/268 [01:33<01:33,  1.43it/s]\u001b[A\n",
            "Iteration:  50% 135/268 [01:34<01:33,  1.43it/s]\u001b[A\n",
            "Iteration:  51% 136/268 [01:35<01:32,  1.43it/s]\u001b[A\n",
            "Iteration:  51% 137/268 [01:36<01:31,  1.42it/s]\u001b[A\n",
            "Iteration:  51% 138/268 [01:36<01:31,  1.43it/s]\u001b[A\n",
            "Iteration:  52% 139/268 [01:37<01:30,  1.43it/s]\u001b[A\n",
            "Iteration:  52% 140/268 [01:38<01:29,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 141/268 [01:38<01:28,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 142/268 [01:39<01:28,  1.43it/s]\u001b[A\n",
            "Iteration:  53% 143/268 [01:40<01:27,  1.43it/s]\u001b[A\n",
            "Iteration:  54% 144/268 [01:40<01:26,  1.43it/s]\u001b[A\n",
            "Iteration:  54% 145/268 [01:41<01:26,  1.43it/s]\u001b[A\n",
            "Iteration:  54% 146/268 [01:42<01:25,  1.43it/s]\u001b[A\n",
            "Iteration:  55% 147/268 [01:43<01:24,  1.43it/s]\u001b[A\n",
            "Iteration:  55% 148/268 [01:43<01:24,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 149/268 [01:44<01:23,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 150/268 [01:45<01:23,  1.42it/s]\u001b[A\n",
            "Iteration:  56% 151/268 [01:45<01:22,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 152/268 [01:46<01:21,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 153/268 [01:47<01:20,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 154/268 [01:47<01:19,  1.43it/s]\u001b[A\n",
            "Iteration:  58% 155/268 [01:48<01:19,  1.42it/s]\u001b[A\n",
            "Iteration:  58% 156/268 [01:49<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 157/268 [01:50<01:18,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 158/268 [01:50<01:17,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 159/268 [01:51<01:16,  1.42it/s]\u001b[A\n",
            "Iteration:  60% 160/268 [01:52<01:16,  1.42it/s]\u001b[A\n",
            "Iteration:  60% 161/268 [01:52<01:15,  1.42it/s]\u001b[A\n",
            "Iteration:  60% 162/268 [01:53<01:14,  1.42it/s]\u001b[A\n",
            "Iteration:  61% 163/268 [01:54<01:14,  1.42it/s]\u001b[A\n",
            "Iteration:  61% 164/268 [01:55<01:13,  1.42it/s]\u001b[A\n",
            "Iteration:  62% 165/268 [01:55<01:12,  1.42it/s]\u001b[A\n",
            "Iteration:  62% 166/268 [01:56<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  62% 167/268 [01:57<01:11,  1.42it/s]\u001b[A\n",
            "Iteration:  63% 168/268 [01:57<01:10,  1.42it/s]\u001b[A\n",
            "Iteration:  63% 169/268 [01:58<01:09,  1.42it/s]\u001b[A\n",
            "Iteration:  63% 170/268 [01:59<01:09,  1.42it/s]\u001b[A\n",
            "Iteration:  64% 171/268 [01:59<01:08,  1.42it/s]\u001b[A\n",
            "Iteration:  64% 172/268 [02:00<01:07,  1.42it/s]\u001b[A\n",
            "Iteration:  65% 173/268 [02:01<01:07,  1.42it/s]\u001b[A\n",
            "Iteration:  65% 174/268 [02:02<01:06,  1.42it/s]\u001b[A\n",
            "Iteration:  65% 175/268 [02:02<01:05,  1.42it/s]\u001b[A\n",
            "Iteration:  66% 176/268 [02:03<01:05,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 177/268 [02:04<01:04,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 178/268 [02:04<01:03,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 179/268 [02:05<01:02,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 180/268 [02:06<01:02,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 181/268 [02:07<01:01,  1.41it/s]\u001b[A\n",
            "Iteration:  68% 182/268 [02:07<01:00,  1.42it/s]\u001b[A\n",
            "Iteration:  68% 183/268 [02:08<01:00,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 184/268 [02:09<00:59,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 185/268 [02:09<00:58,  1.42it/s]\u001b[A\n",
            "Iteration:  69% 186/268 [02:10<00:58,  1.41it/s]\u001b[A\n",
            "Iteration:  70% 187/268 [02:11<00:57,  1.42it/s]\u001b[A\n",
            "Iteration:  70% 188/268 [02:12<00:56,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 189/268 [02:12<00:55,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 190/268 [02:13<00:54,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 191/268 [02:14<00:54,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 192/268 [02:14<00:53,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 193/268 [02:15<00:52,  1.43it/s]\u001b[A\n",
            "Iteration:  72% 194/268 [02:16<00:52,  1.42it/s]\u001b[A\n",
            "Iteration:  73% 195/268 [02:16<00:51,  1.43it/s]\u001b[A\n",
            "Iteration:  73% 196/268 [02:17<00:50,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 197/268 [02:18<00:49,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 198/268 [02:19<00:49,  1.43it/s]\u001b[A\n",
            "Iteration:  74% 199/268 [02:19<00:48,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 200/268 [02:20<00:47,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 201/268 [02:21<00:46,  1.43it/s]\u001b[A\n",
            "Iteration:  75% 202/268 [02:21<00:46,  1.43it/s]\u001b[A\n",
            "Iteration:  76% 203/268 [02:22<00:45,  1.43it/s]\u001b[A\n",
            "Iteration:  76% 204/268 [02:23<00:44,  1.43it/s]\u001b[A\n",
            "Iteration:  76% 205/268 [02:23<00:44,  1.43it/s]\u001b[A\n",
            "Iteration:  77% 206/268 [02:24<00:43,  1.43it/s]\u001b[A\n",
            "Iteration:  77% 207/268 [02:25<00:42,  1.43it/s]\u001b[A\n",
            "Iteration:  78% 208/268 [02:26<00:41,  1.43it/s]\u001b[A\n",
            "Iteration:  78% 209/268 [02:26<00:41,  1.43it/s]\u001b[A\n",
            "Iteration:  78% 210/268 [02:27<00:40,  1.43it/s]\u001b[A\n",
            "Iteration:  79% 211/268 [02:28<00:39,  1.43it/s]\u001b[A\n",
            "Iteration:  79% 212/268 [02:28<00:39,  1.43it/s]\u001b[A\n",
            "Iteration:  79% 213/268 [02:29<00:38,  1.43it/s]\u001b[A\n",
            "Iteration:  80% 214/268 [02:30<00:37,  1.43it/s]\u001b[A\n",
            "Iteration:  80% 215/268 [02:30<00:37,  1.43it/s]\u001b[A\n",
            "Iteration:  81% 216/268 [02:31<00:36,  1.43it/s]\u001b[A\n",
            "Iteration:  81% 217/268 [02:32<00:35,  1.43it/s]\u001b[A\n",
            "Iteration:  81% 218/268 [02:32<00:34,  1.43it/s]\u001b[A\n",
            "Iteration:  82% 219/268 [02:33<00:34,  1.43it/s]\u001b[A\n",
            "Iteration:  82% 220/268 [02:34<00:33,  1.43it/s]\u001b[A\n",
            "Iteration:  82% 221/268 [02:35<00:32,  1.43it/s]\u001b[A\n",
            "Iteration:  83% 222/268 [02:35<00:32,  1.43it/s]\u001b[A\n",
            "Iteration:  83% 223/268 [02:36<00:31,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 224/268 [02:37<00:30,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 225/268 [02:37<00:30,  1.43it/s]\u001b[A\n",
            "Iteration:  84% 226/268 [02:38<00:29,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 227/268 [02:39<00:28,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 228/268 [02:39<00:27,  1.43it/s]\u001b[A\n",
            "Iteration:  85% 229/268 [02:40<00:27,  1.43it/s]\u001b[A\n",
            "Iteration:  86% 230/268 [02:41<00:26,  1.43it/s]\u001b[A\n",
            "Iteration:  86% 231/268 [02:42<00:25,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 232/268 [02:42<00:25,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 233/268 [02:43<00:24,  1.43it/s]\u001b[A\n",
            "Iteration:  87% 234/268 [02:44<00:23,  1.43it/s]\u001b[A\n",
            "Iteration:  88% 235/268 [02:44<00:23,  1.43it/s]\u001b[A\n",
            "Iteration:  88% 236/268 [02:45<00:22,  1.43it/s]\u001b[A\n",
            "Iteration:  88% 237/268 [02:46<00:21,  1.43it/s]\u001b[A\n",
            "Iteration:  89% 238/268 [02:46<00:21,  1.43it/s]\u001b[A\n",
            "Iteration:  89% 239/268 [02:47<00:20,  1.43it/s]\u001b[A\n",
            "Iteration:  90% 240/268 [02:48<00:19,  1.43it/s]\u001b[A\n",
            "Iteration:  90% 241/268 [02:49<00:18,  1.43it/s]\u001b[A\n",
            "Iteration:  90% 242/268 [02:49<00:18,  1.43it/s]\u001b[A\n",
            "Iteration:  91% 243/268 [02:50<00:17,  1.43it/s]\u001b[A\n",
            "Iteration:  91% 244/268 [02:51<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  91% 245/268 [02:51<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  92% 246/268 [02:52<00:15,  1.43it/s]\u001b[A\n",
            "Iteration:  92% 247/268 [02:53<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 248/268 [02:54<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 249/268 [02:54<00:13,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 250/268 [02:55<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 251/268 [02:56<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 252/268 [02:56<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  94% 253/268 [02:57<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 254/268 [02:58<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 255/268 [02:58<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 256/268 [02:59<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 257/268 [03:00<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  96% 258/268 [03:01<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 259/268 [03:01<00:06,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 260/268 [03:02<00:05,  1.42it/s]\u001b[A\n",
            "Iteration:  97% 261/268 [03:03<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 262/268 [03:03<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 263/268 [03:04<00:03,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 264/268 [03:05<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 265/268 [03:05<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  99% 266/268 [03:06<00:01,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 267/268 [03:07<00:00,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 268/268 [03:07<00:00,  1.43it/s]\n",
            "Epoch: 100% 2/2 [06:14<00:00, 187.45s/it]\n",
            "12/02/2020 22:58:50 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:58:50 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:58:50 - INFO - __main__ -   ***** Running evaluation:: Task : cola, Prefix :  *****\n",
            "12/02/2020 22:58:50 - INFO - __main__ -     Num examples = 1043\n",
            "12/02/2020 22:58:50 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  3.95it/s]\n",
            "12/02/2020 22:58:58 - INFO - __main__ -   ***** Eval results  *****\n",
            "{'mcc': 0.555285279196476}\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 0.0, \"loss\": 0.4068444441407919, \"step\": 536}\n",
            "\n",
            "***** Parameters Saved for task 0 *****\n",
            "\n",
            "\n",
            "12/02/2020 22:59:00 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:00 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:59:00 - INFO - __main__ -   ***** Running evaluation:: Task : mrpc, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 22:59:00 - INFO - __main__ -     Num examples = 408\n",
            "12/02/2020 22:59:00 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  3.92it/s]\n",
            "12/02/2020 22:59:04 - INFO - __main__ -   ***** Eval results Future Task (Continual) *****\n",
            "{'acc': 0.6838235294117647, 'f1': 0.8122270742358079, 'acc_and_f1': 0.7480253018237863}\n",
            "12/02/2020 22:59:04 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   Writing example 0/277\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:04 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 22:59:04 - INFO - __main__ -   ***** Running evaluation:: Task : rte, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 22:59:04 - INFO - __main__ -     Num examples = 277\n",
            "12/02/2020 22:59:04 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.05it/s]\n",
            "12/02/2020 22:59:06 - INFO - __main__ -   ***** Eval results Future Task (Continual) *****\n",
            "{'acc': 0.5342960288808665}\n",
            "12/02/2020 22:59:06 - INFO - __main__ -    global_step = 536, average loss = 0.3795190710268581\n",
            "12/02/2020 22:59:07 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   LOOKING AT data/mrpc/train.tsv\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   Writing example 0/3668\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   input_ids: 101 2572 3217 5831 5496 2010 2567 1010 3183 2002 2170 1000 1996 7409 1000 1010 1997 9969 4487 23809 3436 2010 3350 1012 102 7727 2000 2032 2004 2069 1000 1996 7409 1000 1010 2572 3217 5831 5496 2010 2567 1997 9969 4487 23809 3436 2010 3350 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   input_ids: 101 9805 3540 11514 2050 3079 11282 2243 1005 1055 2077 4855 1996 4677 2000 3647 4576 1999 2687 2005 1002 1016 1012 1019 4551 1012 102 9805 3540 11514 2050 4149 11282 2243 1005 1055 1999 2786 2005 1002 6353 2509 2454 1998 2853 2009 2000 3647 4576 2005 1002 1015 1012 1022 4551 1999 2687 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   input_ids: 101 2027 2018 2405 2019 15147 2006 1996 4274 2006 2238 2184 1010 5378 1996 6636 2005 5096 1010 2002 2794 1012 102 2006 2238 2184 1010 1996 2911 1005 1055 5608 2018 2405 2019 15147 2006 1996 4274 1010 5378 1996 14792 2005 5096 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   input_ids: 101 2105 6021 19481 13938 2102 1010 21628 6661 2020 2039 2539 16653 1010 2030 1018 1012 1018 1003 1010 2012 1037 1002 1018 1012 5179 1010 2383 3041 2275 1037 2501 2152 1997 1037 1002 1018 1012 5401 1012 102 21628 6661 5598 2322 16653 1010 2030 1018 1012 1020 1003 1010 2000 2275 1037 2501 5494 2152 2012 1037 1002 1018 1012 5401 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 4518 3123 1002 1016 1012 2340 1010 2030 2055 2340 3867 1010 2000 2485 5958 2012 1002 2538 1012 4868 2006 1996 2047 2259 4518 3863 1012 102 18720 1004 1041 13058 1012 6661 5598 1002 1015 1012 6191 2030 1022 3867 2000 1002 2538 1012 6021 2006 1996 2047 2259 4518 3863 2006 5958 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 22:59:07 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 22:59:10 - INFO - __main__ -   ***** Running training *****\n",
            "12/02/2020 22:59:10 - INFO - __main__ -     Num examples = 3668\n",
            "12/02/2020 22:59:10 - INFO - __main__ -     Num Epochs = 2\n",
            "12/02/2020 22:59:10 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "12/02/2020 22:59:10 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "12/02/2020 22:59:10 - INFO - __main__ -     Total optimization steps = 230\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:27,  1.31it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:25,  1.32it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:23,  1.35it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:21,  1.36it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:20,  1.36it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:19,  1.37it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:17,  1.37it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:17,  1.37it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:16,  1.38it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:15,  1.38it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.38it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:11,  1.37it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:09,  1.37it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:08,  1.36it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:06,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:18<01:05,  1.36it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:04,  1.36it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:03,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:21<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:02,  1.35it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:01,  1.34it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<01:00,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:24<01:00,  1.35it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:59,  1.35it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:58,  1.34it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:27<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:27<00:57,  1.35it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:55,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:30<00:54,  1.35it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:53,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:33<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:49,  1.37it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:48,  1.37it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:35<00:48,  1.37it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:47,  1.37it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:46,  1.37it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:38<00:45,  1.37it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:38<00:44,  1.38it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:44,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:43,  1.38it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:41<00:42,  1.38it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:42,  1.38it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:41,  1.38it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:40,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:43<00:39,  1.38it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:39,  1.38it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:38,  1.38it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:46<00:37,  1.38it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:46<00:36,  1.38it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:36,  1.38it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:35,  1.38it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:49<00:34,  1.38it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:49<00:33,  1.38it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:33,  1.38it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:32,  1.38it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:51<00:31,  1.38it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:52<00:31,  1.38it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.38it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:54<00:29,  1.39it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:54<00:28,  1.39it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:55<00:28,  1.39it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:27,  1.39it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:56<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:57<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:58<00:25,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:24,  1.38it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:59<00:23,  1.38it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:00<00:23,  1.38it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:22,  1.38it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:02<00:21,  1.38it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:02<00:20,  1.39it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:03<00:20,  1.38it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.38it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:04<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:05<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:06<00:17,  1.38it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:16,  1.38it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:07<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:08<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:09<00:14,  1.38it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:09<00:13,  1.38it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:10<00:13,  1.38it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:11<00:12,  1.38it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:12<00:11,  1.38it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:12<00:10,  1.38it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:13<00:10,  1.38it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:14<00:09,  1.38it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:15<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:15<00:07,  1.38it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:16<00:07,  1.37it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:17<00:06,  1.37it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:17<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:18<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:19<00:04,  1.37it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:20<00:03,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:20<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:21<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:22<00:01,  1.37it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:23<00:00,  1.37it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:23<00:00,  1.38it/s]\n",
            "Epoch:  50% 1/2 [01:23<01:23, 83.58s/it]\n",
            "Iteration:   0% 0/115 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/115 [00:00<01:23,  1.37it/s]\u001b[A\n",
            "Iteration:   2% 2/115 [00:01<01:22,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 3/115 [00:02<01:21,  1.37it/s]\u001b[A\n",
            "Iteration:   3% 4/115 [00:02<01:21,  1.37it/s]\u001b[A\n",
            "Iteration:   4% 5/115 [00:03<01:20,  1.37it/s]\u001b[A\n",
            "Iteration:   5% 6/115 [00:04<01:19,  1.37it/s]\u001b[A\n",
            "Iteration:   6% 7/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   7% 8/115 [00:05<01:18,  1.37it/s]\u001b[A\n",
            "Iteration:   8% 9/115 [00:06<01:17,  1.37it/s]\u001b[A\n",
            "Iteration:   9% 10/115 [00:07<01:16,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 11/115 [00:08<01:15,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 12/115 [00:08<01:14,  1.37it/s]\u001b[A\n",
            "Iteration:  11% 13/115 [00:09<01:14,  1.37it/s]\u001b[A\n",
            "Iteration:  12% 14/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  13% 15/115 [00:10<01:13,  1.37it/s]\u001b[A\n",
            "Iteration:  14% 16/115 [00:11<01:12,  1.37it/s]\u001b[A\n",
            "Iteration:  15% 17/115 [00:12<01:11,  1.37it/s]\u001b[A\n",
            "Iteration:  16% 18/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 19/115 [00:13<01:10,  1.37it/s]\u001b[A\n",
            "Iteration:  17% 20/115 [00:14<01:09,  1.37it/s]\u001b[A\n",
            "Iteration:  18% 21/115 [00:15<01:08,  1.37it/s]\u001b[A\n",
            "Iteration:  19% 22/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  20% 23/115 [00:16<01:07,  1.37it/s]\u001b[A\n",
            "Iteration:  21% 24/115 [00:17<01:06,  1.37it/s]\u001b[A\n",
            "Iteration:  22% 25/115 [00:18<01:05,  1.37it/s]\u001b[A\n",
            "Iteration:  23% 26/115 [00:18<01:04,  1.37it/s]\u001b[A\n",
            "Iteration:  23% 27/115 [00:19<01:04,  1.37it/s]\u001b[A\n",
            "Iteration:  24% 28/115 [00:20<01:03,  1.37it/s]\u001b[A\n",
            "Iteration:  25% 29/115 [00:21<01:02,  1.37it/s]\u001b[A\n",
            "Iteration:  26% 30/115 [00:21<01:02,  1.37it/s]\u001b[A\n",
            "Iteration:  27% 31/115 [00:22<01:01,  1.37it/s]\u001b[A\n",
            "Iteration:  28% 32/115 [00:23<01:00,  1.37it/s]\u001b[A\n",
            "Iteration:  29% 33/115 [00:24<00:59,  1.37it/s]\u001b[A\n",
            "Iteration:  30% 34/115 [00:24<00:59,  1.37it/s]\u001b[A\n",
            "Iteration:  30% 35/115 [00:25<00:58,  1.37it/s]\u001b[A\n",
            "Iteration:  31% 36/115 [00:26<00:57,  1.37it/s]\u001b[A\n",
            "Iteration:  32% 37/115 [00:26<00:57,  1.37it/s]\u001b[A\n",
            "Iteration:  33% 38/115 [00:27<00:56,  1.37it/s]\u001b[A\n",
            "Iteration:  34% 39/115 [00:28<00:55,  1.37it/s]\u001b[A\n",
            "Iteration:  35% 40/115 [00:29<00:54,  1.37it/s]\u001b[A\n",
            "Iteration:  36% 41/115 [00:29<00:53,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 42/115 [00:30<00:53,  1.37it/s]\u001b[A\n",
            "Iteration:  37% 43/115 [00:31<00:52,  1.37it/s]\u001b[A\n",
            "Iteration:  38% 44/115 [00:32<00:51,  1.37it/s]\u001b[A\n",
            "Iteration:  39% 45/115 [00:32<00:50,  1.37it/s]\u001b[A\n",
            "Iteration:  40% 46/115 [00:33<00:50,  1.37it/s]\u001b[A\n",
            "Iteration:  41% 47/115 [00:34<00:49,  1.37it/s]\u001b[A\n",
            "Iteration:  42% 48/115 [00:35<00:48,  1.37it/s]\u001b[A\n",
            "Iteration:  43% 49/115 [00:35<00:48,  1.37it/s]\u001b[A\n",
            "Iteration:  43% 50/115 [00:36<00:47,  1.37it/s]\u001b[A\n",
            "Iteration:  44% 51/115 [00:37<00:46,  1.37it/s]\u001b[A\n",
            "Iteration:  45% 52/115 [00:37<00:45,  1.37it/s]\u001b[A\n",
            "Iteration:  46% 53/115 [00:38<00:45,  1.37it/s]\u001b[A\n",
            "Iteration:  47% 54/115 [00:39<00:44,  1.37it/s]\u001b[A\n",
            "Iteration:  48% 55/115 [00:40<00:43,  1.37it/s]\u001b[A\n",
            "Iteration:  49% 56/115 [00:40<00:42,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 57/115 [00:41<00:42,  1.37it/s]\u001b[A\n",
            "Iteration:  50% 58/115 [00:42<00:41,  1.37it/s]\u001b[A\n",
            "Iteration:  51% 59/115 [00:43<00:40,  1.38it/s]\u001b[A\n",
            "Iteration:  52% 60/115 [00:43<00:40,  1.37it/s]\u001b[A\n",
            "Iteration:  53% 61/115 [00:44<00:39,  1.37it/s]\u001b[A\n",
            "Iteration:  54% 62/115 [00:45<00:38,  1.37it/s]\u001b[A\n",
            "Iteration:  55% 63/115 [00:45<00:37,  1.37it/s]\u001b[A\n",
            "Iteration:  56% 64/115 [00:46<00:37,  1.37it/s]\u001b[A\n",
            "Iteration:  57% 65/115 [00:47<00:36,  1.38it/s]\u001b[A\n",
            "Iteration:  57% 66/115 [00:48<00:35,  1.37it/s]\u001b[A\n",
            "Iteration:  58% 67/115 [00:48<00:35,  1.37it/s]\u001b[A\n",
            "Iteration:  59% 68/115 [00:49<00:34,  1.37it/s]\u001b[A\n",
            "Iteration:  60% 69/115 [00:50<00:33,  1.37it/s]\u001b[A\n",
            "Iteration:  61% 70/115 [00:51<00:32,  1.37it/s]\u001b[A\n",
            "Iteration:  62% 71/115 [00:51<00:32,  1.37it/s]\u001b[A\n",
            "Iteration:  63% 72/115 [00:52<00:31,  1.37it/s]\u001b[A\n",
            "Iteration:  63% 73/115 [00:53<00:30,  1.37it/s]\u001b[A\n",
            "Iteration:  64% 74/115 [00:53<00:29,  1.37it/s]\u001b[A\n",
            "Iteration:  65% 75/115 [00:54<00:29,  1.37it/s]\u001b[A\n",
            "Iteration:  66% 76/115 [00:55<00:28,  1.37it/s]\u001b[A\n",
            "Iteration:  67% 77/115 [00:56<00:27,  1.37it/s]\u001b[A\n",
            "Iteration:  68% 78/115 [00:56<00:26,  1.37it/s]\u001b[A\n",
            "Iteration:  69% 79/115 [00:57<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 80/115 [00:58<00:25,  1.38it/s]\u001b[A\n",
            "Iteration:  70% 81/115 [00:59<00:24,  1.38it/s]\u001b[A\n",
            "Iteration:  71% 82/115 [00:59<00:24,  1.37it/s]\u001b[A\n",
            "Iteration:  72% 83/115 [01:00<00:23,  1.38it/s]\u001b[A\n",
            "Iteration:  73% 84/115 [01:01<00:22,  1.38it/s]\u001b[A\n",
            "Iteration:  74% 85/115 [01:01<00:21,  1.38it/s]\u001b[A\n",
            "Iteration:  75% 86/115 [01:02<00:21,  1.37it/s]\u001b[A\n",
            "Iteration:  76% 87/115 [01:03<00:20,  1.38it/s]\u001b[A\n",
            "Iteration:  77% 88/115 [01:04<00:19,  1.38it/s]\u001b[A\n",
            "Iteration:  77% 89/115 [01:04<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  78% 90/115 [01:05<00:18,  1.38it/s]\u001b[A\n",
            "Iteration:  79% 91/115 [01:06<00:17,  1.38it/s]\u001b[A\n",
            "Iteration:  80% 92/115 [01:07<00:16,  1.38it/s]\u001b[A\n",
            "Iteration:  81% 93/115 [01:07<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  82% 94/115 [01:08<00:15,  1.38it/s]\u001b[A\n",
            "Iteration:  83% 95/115 [01:09<00:14,  1.38it/s]\u001b[A\n",
            "Iteration:  83% 96/115 [01:09<00:13,  1.38it/s]\u001b[A\n",
            "Iteration:  84% 97/115 [01:10<00:13,  1.38it/s]\u001b[A\n",
            "Iteration:  85% 98/115 [01:11<00:12,  1.38it/s]\u001b[A\n",
            "Iteration:  86% 99/115 [01:12<00:11,  1.38it/s]\u001b[A\n",
            "Iteration:  87% 100/115 [01:12<00:10,  1.38it/s]\u001b[A\n",
            "Iteration:  88% 101/115 [01:13<00:10,  1.38it/s]\u001b[A\n",
            "Iteration:  89% 102/115 [01:14<00:09,  1.38it/s]\u001b[A\n",
            "Iteration:  90% 103/115 [01:15<00:08,  1.38it/s]\u001b[A\n",
            "Iteration:  90% 104/115 [01:15<00:07,  1.38it/s]\u001b[A\n",
            "Iteration:  91% 105/115 [01:16<00:07,  1.38it/s]\u001b[A\n",
            "Iteration:  92% 106/115 [01:17<00:06,  1.38it/s]\u001b[A\n",
            "Iteration:  93% 107/115 [01:17<00:05,  1.38it/s]\u001b[A\n",
            "Iteration:  94% 108/115 [01:18<00:05,  1.38it/s]\u001b[A\n",
            "Iteration:  95% 109/115 [01:19<00:04,  1.38it/s]\u001b[A\n",
            "Iteration:  96% 110/115 [01:20<00:03,  1.38it/s]\u001b[A\n",
            "Iteration:  97% 111/115 [01:20<00:02,  1.39it/s]\u001b[A\n",
            "Iteration:  97% 112/115 [01:21<00:02,  1.39it/s]\u001b[A\n",
            "Iteration:  98% 113/115 [01:22<00:01,  1.38it/s]\u001b[A\n",
            "Iteration:  99% 114/115 [01:22<00:00,  1.38it/s]\u001b[A\n",
            "Iteration: 100% 115/115 [01:23<00:00,  1.38it/s]\n",
            "Epoch: 100% 2/2 [02:47<00:00, 83.51s/it]\n",
            "12/02/2020 23:01:57 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:01:57 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:01:57 - INFO - __main__ -   ***** Running evaluation:: Task : mrpc, Prefix :  *****\n",
            "12/02/2020 23:01:57 - INFO - __main__ -     Num examples = 408\n",
            "12/02/2020 23:01:57 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  4.03it/s]\n",
            "12/02/2020 23:02:01 - INFO - __main__ -   ***** Eval results  *****\n",
            "{'acc': 0.8235294117647058, 'f1': 0.8779661016949152, 'acc_and_f1': 0.8507477567298105}\n",
            "{\"learning_rate\": 0.0, \"loss\": 0.19297558744251728, \"step\": 230}\n",
            "\n",
            "***** Parameters Saved for task 1 *****\n",
            "\n",
            "\n",
            "12/02/2020 23:02:02 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:02:02 - INFO - __main__ -   ***** Running evaluation:: Task : cola, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 23:02:02 - INFO - __main__ -     Num examples = 1043\n",
            "12/02/2020 23:02:02 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  4.00it/s]\n",
            "12/02/2020 23:02:11 - INFO - __main__ -   ***** Eval results Previous Task (Continual) *****\n",
            "{'mcc': 0.5491398222815213}\n",
            "12/02/2020 23:02:11 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   Writing example 0/277\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:11 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:02:11 - INFO - __main__ -   ***** Running evaluation:: Task : rte, Prefix : Future Task (Continual) *****\n",
            "12/02/2020 23:02:11 - INFO - __main__ -     Num examples = 277\n",
            "12/02/2020 23:02:11 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.08it/s]\n",
            "12/02/2020 23:02:13 - INFO - __main__ -   ***** Eval results Future Task (Continual) *****\n",
            "{'acc': 0.49458483754512633}\n",
            "12/02/2020 23:02:13 - INFO - __main__ -    global_step = 230, average loss = 0.419512146614168\n",
            "12/02/2020 23:02:13 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   Writing example 0/2490\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   input_ids: 101 1037 2173 1997 14038 1010 2044 4831 2198 2703 2462 2351 1010 2150 1037 2173 1997 7401 1010 2004 3142 3234 11633 5935 1999 5116 3190 2000 2928 1996 8272 1997 2047 4831 12122 16855 1012 102 4831 12122 16855 2003 1996 2047 3003 1997 1996 3142 3234 2277 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   input_ids: 101 2014 3401 13876 2378 2001 2525 4844 2000 7438 1996 5305 4355 7388 4456 5022 1010 1998 1996 2194 2056 1010 6928 1010 2009 2097 6848 2007 2976 25644 1996 6061 1997 3653 11020 3089 10472 1996 4319 2005 2062 7388 4456 5022 1012 102 2014 3401 13876 2378 2064 2022 2109 2000 7438 7388 4456 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   input_ids: 101 18414 10265 13801 1010 2708 3237 2012 20877 2098 5555 1010 1037 2966 2326 2194 2008 7126 15770 1996 1016 1011 2095 1011 2214 5148 2540 2820 1999 7570 9610 19538 2103 1006 3839 24001 1007 1010 2056 2008 2061 2521 2055 1015 1010 3156 2336 2031 2363 3949 1012 102 1996 3025 2171 1997 7570 9610 19538 2103 2001 24001 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   input_ids: 101 1037 2158 2003 2349 1999 2457 2101 5338 2007 1996 4028 2656 2086 3283 1997 1037 10563 3005 2553 2001 1996 2034 2000 2022 2956 2006 4035 2028 1005 1055 4126 18866 1012 5624 4674 19027 2213 1010 2385 1010 2001 3788 2000 2014 6898 1005 1055 2160 1999 3145 5172 1010 20126 1010 2006 2382 2255 3172 2043 2016 5419 1012 2014 2303 2001 2101 2179 1999 1037 2492 2485 2000 2014 2188 1012 2703 5954 17165 1010 2753 1010 2038 2042 5338 2007 4028 1998 2003 2349 2077 11331 23007 2101 1012 102 2703 5954 17165 2003 5496 1997 2383 13263 1037 2611 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:02:13 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:02:16 - INFO - __main__ -   ***** Running training *****\n",
            "12/02/2020 23:02:16 - INFO - __main__ -     Num examples = 2490\n",
            "12/02/2020 23:02:16 - INFO - __main__ -     Num Epochs = 2\n",
            "12/02/2020 23:02:16 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "12/02/2020 23:02:16 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "12/02/2020 23:02:16 - INFO - __main__ -     Total optimization steps = 156\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/78 [00:00<00:57,  1.34it/s]\u001b[A\n",
            "Iteration:   3% 2/78 [00:01<00:56,  1.35it/s]\u001b[A\n",
            "Iteration:   4% 3/78 [00:02<00:55,  1.35it/s]\u001b[A\n",
            "Iteration:   5% 4/78 [00:02<00:54,  1.36it/s]\u001b[A\n",
            "Iteration:   6% 5/78 [00:03<00:53,  1.36it/s]\u001b[A\n",
            "Iteration:   8% 6/78 [00:04<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:   9% 7/78 [00:05<00:52,  1.36it/s]\u001b[A\n",
            "Iteration:  10% 8/78 [00:05<00:51,  1.36it/s]\u001b[A\n",
            "Iteration:  12% 9/78 [00:06<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  13% 10/78 [00:07<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  14% 11/78 [00:08<00:49,  1.35it/s]\u001b[A\n",
            "Iteration:  15% 12/78 [00:08<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  17% 13/78 [00:09<00:48,  1.35it/s]\u001b[A\n",
            "Iteration:  18% 14/78 [00:10<00:47,  1.35it/s]\u001b[A\n",
            "Iteration:  19% 15/78 [00:11<00:46,  1.35it/s]\u001b[A\n",
            "Iteration:  21% 16/78 [00:11<00:46,  1.34it/s]\u001b[A\n",
            "Iteration:  22% 17/78 [00:12<00:45,  1.34it/s]\u001b[A\n",
            "Iteration:  23% 18/78 [00:13<00:44,  1.34it/s]\u001b[A\n",
            "Iteration:  24% 19/78 [00:14<00:44,  1.33it/s]\u001b[A\n",
            "Iteration:  26% 20/78 [00:14<00:43,  1.34it/s]\u001b[A\n",
            "Iteration:  27% 21/78 [00:15<00:42,  1.34it/s]\u001b[A\n",
            "Iteration:  28% 22/78 [00:16<00:41,  1.33it/s]\u001b[A\n",
            "Iteration:  29% 23/78 [00:17<00:41,  1.33it/s]\u001b[A\n",
            "Iteration:  31% 24/78 [00:17<00:40,  1.33it/s]\u001b[A\n",
            "Iteration:  32% 25/78 [00:18<00:39,  1.33it/s]\u001b[A\n",
            "Iteration:  33% 26/78 [00:19<00:39,  1.33it/s]\u001b[A\n",
            "Iteration:  35% 27/78 [00:20<00:38,  1.32it/s]\u001b[A\n",
            "Iteration:  36% 28/78 [00:20<00:37,  1.33it/s]\u001b[A\n",
            "Iteration:  37% 29/78 [00:21<00:36,  1.33it/s]\u001b[A\n",
            "Iteration:  38% 30/78 [00:22<00:36,  1.33it/s]\u001b[A\n",
            "Iteration:  40% 31/78 [00:23<00:35,  1.33it/s]\u001b[A\n",
            "Iteration:  41% 32/78 [00:23<00:34,  1.33it/s]\u001b[A\n",
            "Iteration:  42% 33/78 [00:24<00:33,  1.33it/s]\u001b[A\n",
            "Iteration:  44% 34/78 [00:25<00:32,  1.34it/s]\u001b[A\n",
            "Iteration:  45% 35/78 [00:26<00:32,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 36/78 [00:26<00:31,  1.34it/s]\u001b[A\n",
            "Iteration:  47% 37/78 [00:27<00:30,  1.34it/s]\u001b[A\n",
            "Iteration:  49% 38/78 [00:28<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 39/78 [00:29<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 40/78 [00:29<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 41/78 [00:30<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 42/78 [00:31<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 43/78 [00:32<00:25,  1.36it/s]\u001b[A\n",
            "Iteration:  56% 44/78 [00:32<00:25,  1.36it/s]\u001b[A\n",
            "Iteration:  58% 45/78 [00:33<00:24,  1.36it/s]\u001b[A\n",
            "Iteration:  59% 46/78 [00:34<00:23,  1.36it/s]\u001b[A\n",
            "Iteration:  60% 47/78 [00:34<00:22,  1.37it/s]\u001b[A\n",
            "Iteration:  62% 48/78 [00:35<00:22,  1.36it/s]\u001b[A\n",
            "Iteration:  63% 49/78 [00:36<00:21,  1.36it/s]\u001b[A\n",
            "Iteration:  64% 50/78 [00:37<00:20,  1.37it/s]\u001b[A\n",
            "Iteration:  65% 51/78 [00:37<00:19,  1.37it/s]\u001b[A\n",
            "Iteration:  67% 52/78 [00:38<00:19,  1.37it/s]\u001b[A\n",
            "Iteration:  68% 53/78 [00:39<00:18,  1.37it/s]\u001b[A\n",
            "Iteration:  69% 54/78 [00:40<00:17,  1.37it/s]\u001b[A\n",
            "Iteration:  71% 55/78 [00:40<00:16,  1.37it/s]\u001b[A\n",
            "Iteration:  72% 56/78 [00:41<00:16,  1.37it/s]\u001b[A\n",
            "Iteration:  73% 57/78 [00:42<00:15,  1.37it/s]\u001b[A\n",
            "Iteration:  74% 58/78 [00:42<00:14,  1.37it/s]\u001b[A\n",
            "Iteration:  76% 59/78 [00:43<00:13,  1.37it/s]\u001b[A\n",
            "Iteration:  77% 60/78 [00:44<00:13,  1.37it/s]\u001b[A\n",
            "Iteration:  78% 61/78 [00:45<00:12,  1.37it/s]\u001b[A\n",
            "Iteration:  79% 62/78 [00:45<00:11,  1.37it/s]\u001b[A\n",
            "Iteration:  81% 63/78 [00:46<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  82% 64/78 [00:47<00:10,  1.37it/s]\u001b[A\n",
            "Iteration:  83% 65/78 [00:48<00:09,  1.38it/s]\u001b[A\n",
            "Iteration:  85% 66/78 [00:48<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  86% 67/78 [00:49<00:08,  1.37it/s]\u001b[A\n",
            "Iteration:  87% 68/78 [00:50<00:07,  1.37it/s]\u001b[A\n",
            "Iteration:  88% 69/78 [00:51<00:06,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 70/78 [00:51<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  91% 71/78 [00:52<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  92% 72/78 [00:53<00:04,  1.37it/s]\u001b[A\n",
            "Iteration:  94% 73/78 [00:53<00:03,  1.37it/s]\u001b[A\n",
            "Iteration:  95% 74/78 [00:54<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  96% 75/78 [00:55<00:02,  1.38it/s]\u001b[A\n",
            "Iteration:  97% 76/78 [00:56<00:01,  1.37it/s]\u001b[A\n",
            "Iteration:  99% 77/78 [00:56<00:00,  1.37it/s]\u001b[A\n",
            "Iteration: 100% 78/78 [00:57<00:00,  1.36it/s]\n",
            "Epoch:  50% 1/2 [00:57<00:57, 57.44s/it]\n",
            "Iteration:   0% 0/78 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/78 [00:00<00:55,  1.38it/s]\u001b[A\n",
            "Iteration:   3% 2/78 [00:01<00:55,  1.38it/s]\u001b[A\n",
            "Iteration:   4% 3/78 [00:02<00:54,  1.38it/s]\u001b[A\n",
            "Iteration:   5% 4/78 [00:02<00:53,  1.37it/s]\u001b[A\n",
            "Iteration:   6% 5/78 [00:03<00:53,  1.37it/s]\u001b[A\n",
            "Iteration:   8% 6/78 [00:04<00:52,  1.37it/s]\u001b[A\n",
            "Iteration:   9% 7/78 [00:05<00:51,  1.37it/s]\u001b[A\n",
            "Iteration:  10% 8/78 [00:05<00:51,  1.37it/s]\u001b[A\n",
            "Iteration:  12% 9/78 [00:06<00:50,  1.36it/s]\u001b[A\n",
            "Iteration:  13% 10/78 [00:07<00:49,  1.36it/s]\u001b[A\n",
            "Iteration:  14% 11/78 [00:08<00:49,  1.37it/s]\u001b[A\n",
            "Iteration:  15% 12/78 [00:08<00:48,  1.36it/s]\u001b[A\n",
            "Iteration:  17% 13/78 [00:09<00:47,  1.36it/s]\u001b[A\n",
            "Iteration:  18% 14/78 [00:10<00:47,  1.36it/s]\u001b[A\n",
            "Iteration:  19% 15/78 [00:11<00:46,  1.36it/s]\u001b[A\n",
            "Iteration:  21% 16/78 [00:11<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  22% 17/78 [00:12<00:45,  1.35it/s]\u001b[A\n",
            "Iteration:  23% 18/78 [00:13<00:44,  1.36it/s]\u001b[A\n",
            "Iteration:  24% 19/78 [00:13<00:43,  1.35it/s]\u001b[A\n",
            "Iteration:  26% 20/78 [00:14<00:42,  1.36it/s]\u001b[A\n",
            "Iteration:  27% 21/78 [00:15<00:42,  1.36it/s]\u001b[A\n",
            "Iteration:  28% 22/78 [00:16<00:41,  1.36it/s]\u001b[A\n",
            "Iteration:  29% 23/78 [00:16<00:40,  1.36it/s]\u001b[A\n",
            "Iteration:  31% 24/78 [00:17<00:39,  1.36it/s]\u001b[A\n",
            "Iteration:  32% 25/78 [00:18<00:39,  1.35it/s]\u001b[A\n",
            "Iteration:  33% 26/78 [00:19<00:38,  1.35it/s]\u001b[A\n",
            "Iteration:  35% 27/78 [00:19<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  36% 28/78 [00:20<00:37,  1.34it/s]\u001b[A\n",
            "Iteration:  37% 29/78 [00:21<00:36,  1.35it/s]\u001b[A\n",
            "Iteration:  38% 30/78 [00:22<00:35,  1.35it/s]\u001b[A\n",
            "Iteration:  40% 31/78 [00:22<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  41% 32/78 [00:23<00:34,  1.35it/s]\u001b[A\n",
            "Iteration:  42% 33/78 [00:24<00:33,  1.35it/s]\u001b[A\n",
            "Iteration:  44% 34/78 [00:25<00:32,  1.35it/s]\u001b[A\n",
            "Iteration:  45% 35/78 [00:25<00:31,  1.34it/s]\u001b[A\n",
            "Iteration:  46% 36/78 [00:26<00:31,  1.35it/s]\u001b[A\n",
            "Iteration:  47% 37/78 [00:27<00:30,  1.35it/s]\u001b[A\n",
            "Iteration:  49% 38/78 [00:28<00:29,  1.35it/s]\u001b[A\n",
            "Iteration:  50% 39/78 [00:28<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  51% 40/78 [00:29<00:28,  1.35it/s]\u001b[A\n",
            "Iteration:  53% 41/78 [00:30<00:27,  1.35it/s]\u001b[A\n",
            "Iteration:  54% 42/78 [00:30<00:26,  1.35it/s]\u001b[A\n",
            "Iteration:  55% 43/78 [00:31<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  56% 44/78 [00:32<00:25,  1.35it/s]\u001b[A\n",
            "Iteration:  58% 45/78 [00:33<00:24,  1.35it/s]\u001b[A\n",
            "Iteration:  59% 46/78 [00:33<00:23,  1.35it/s]\u001b[A\n",
            "Iteration:  60% 47/78 [00:34<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  62% 48/78 [00:35<00:22,  1.35it/s]\u001b[A\n",
            "Iteration:  63% 49/78 [00:36<00:21,  1.35it/s]\u001b[A\n",
            "Iteration:  64% 50/78 [00:36<00:20,  1.35it/s]\u001b[A\n",
            "Iteration:  65% 51/78 [00:37<00:19,  1.36it/s]\u001b[A\n",
            "Iteration:  67% 52/78 [00:38<00:19,  1.36it/s]\u001b[A\n",
            "Iteration:  68% 53/78 [00:39<00:18,  1.36it/s]\u001b[A\n",
            "Iteration:  69% 54/78 [00:39<00:17,  1.36it/s]\u001b[A\n",
            "Iteration:  71% 55/78 [00:40<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  72% 56/78 [00:41<00:16,  1.36it/s]\u001b[A\n",
            "Iteration:  73% 57/78 [00:42<00:15,  1.36it/s]\u001b[A\n",
            "Iteration:  74% 58/78 [00:42<00:14,  1.36it/s]\u001b[A\n",
            "Iteration:  76% 59/78 [00:43<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  77% 60/78 [00:44<00:13,  1.36it/s]\u001b[A\n",
            "Iteration:  78% 61/78 [00:44<00:12,  1.36it/s]\u001b[A\n",
            "Iteration:  79% 62/78 [00:45<00:11,  1.36it/s]\u001b[A\n",
            "Iteration:  81% 63/78 [00:46<00:11,  1.36it/s]\u001b[A\n",
            "Iteration:  82% 64/78 [00:47<00:10,  1.36it/s]\u001b[A\n",
            "Iteration:  83% 65/78 [00:47<00:09,  1.36it/s]\u001b[A\n",
            "Iteration:  85% 66/78 [00:48<00:08,  1.36it/s]\u001b[A\n",
            "Iteration:  86% 67/78 [00:49<00:08,  1.36it/s]\u001b[A\n",
            "Iteration:  87% 68/78 [00:50<00:07,  1.37it/s]\u001b[A\n",
            "Iteration:  88% 69/78 [00:50<00:06,  1.36it/s]\u001b[A\n",
            "Iteration:  90% 70/78 [00:51<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  91% 71/78 [00:52<00:05,  1.37it/s]\u001b[A\n",
            "Iteration:  92% 72/78 [00:53<00:04,  1.37it/s]\u001b[A\n",
            "Iteration:  94% 73/78 [00:53<00:03,  1.37it/s]\u001b[A\n",
            "Iteration:  95% 74/78 [00:54<00:02,  1.38it/s]\u001b[A\n",
            "Iteration:  96% 75/78 [00:55<00:02,  1.37it/s]\u001b[A\n",
            "Iteration:  97% 76/78 [00:55<00:01,  1.38it/s]\u001b[A\n",
            "Iteration:  99% 77/78 [00:56<00:00,  1.38it/s]\u001b[A\n",
            "Iteration: 100% 78/78 [00:57<00:00,  1.36it/s]\n",
            "Epoch: 100% 2/2 [01:54<00:00, 57.36s/it]\n",
            "12/02/2020 23:04:11 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   Writing example 0/277\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   input_ids: 101 2664 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 4295 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 10327 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   input_ids: 101 11096 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 1999 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 2009 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 2021 1999 2070 2752 2027 2024 3554 2067 1012 1999 18454 10024 1010 2028 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 11096 1012 102\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 26445 4095 2451 1999 3552 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 1998 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 2137 2554 1012 2021 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 2065 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 5180 2332 1010 102 3552 2038 1996 5221 26445 4095 2451 1999 1996 1057 1012 1055 1012 102\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   input_ids: 101 3036 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 3036 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:11 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "12/02/2020 23:04:11 - INFO - __main__ -   ***** Running evaluation:: Task : rte, Prefix :  *****\n",
            "12/02/2020 23:04:11 - INFO - __main__ -     Num examples = 277\n",
            "12/02/2020 23:04:11 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 9/9 [00:02<00:00,  4.07it/s]\n",
            "12/02/2020 23:04:14 - INFO - __main__ -   ***** Eval results  *****\n",
            "{'acc': 0.6678700361010831}\n",
            "{\"learning_rate\": 0.0, \"loss\": 0.1936706789135933, \"step\": 156}\n",
            "\n",
            "***** Parameters Saved for task 2 *****\n",
            "\n",
            "\n",
            "12/02/2020 23:04:15 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 11279 8469 1996 9478 3154 1997 1996 5749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 15871 2081 1996 8164 7683 2058 1996 4139 3240 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 6228 10658 23277 8004 11533 2993 6065 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   input_ids: 101 2065 2017 2018 8828 2062 1010 2017 2052 2215 2625 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   input_ids: 101 2004 2017 4521 1996 2087 1010 2017 2215 1996 2560 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:15 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:04:15 - INFO - __main__ -   ***** Running evaluation:: Task : cola, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 23:04:15 - INFO - __main__ -     Num examples = 1043\n",
            "12/02/2020 23:04:15 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 33/33 [00:08<00:00,  4.01it/s]\n",
            "12/02/2020 23:04:24 - INFO - __main__ -   ***** Eval results Previous Task (Continual) *****\n",
            "{'mcc': 0.5207572018426233}\n",
            "12/02/2020 23:04:24 - INFO - __main__ -   Creating features from dataset file at data\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   input_ids: 101 2002 2056 1996 9440 2121 7903 2063 11345 2449 2987 1005 1056 4906 1996 2194 1005 1055 2146 1011 2744 3930 5656 1012 102 1000 1996 9440 2121 7903 2063 11345 2449 2515 2025 4906 2256 2146 1011 2744 3930 5656 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   input_ids: 101 20201 22948 2056 10958 19053 4140 6283 1996 8956 6939 1998 2246 2830 2000 2478 2010 2146 2086 1997 2731 1999 1996 2162 1012 102 2010 2564 2056 2002 2001 1000 2531 3867 2369 2577 5747 1000 1998 2246 2830 2000 2478 2010 2086 1997 2731 1999 1996 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 7922 2001 2012 12904 1012 6227 18371 2114 1996 18371 1010 4257 2006 1996 5219 1010 1998 2012 1015 1012 27054 2487 2114 1996 5364 23151 2278 1010 2036 4257 1012 102 1996 7922 2001 2012 12904 1012 6275 18371 16545 2100 1027 1010 8990 4257 2006 1996 5219 1010 1998 2012 1015 1012 23090 2487 2114 1996 5364 23151 2278 10381 2546 1027 1010 2091 1014 1012 1015 3867 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   input_ids: 101 1996 10028 1011 25022 2080 2003 3403 2127 2255 2000 5630 2065 2009 2097 2203 5668 2063 1037 4018 1012 102 1996 10028 1011 25022 2080 2623 9317 2008 2009 2097 5630 1999 2255 3251 2000 2203 5668 2063 1037 4018 2077 1996 27419 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   input_ids: 101 2053 5246 2031 2042 2275 2005 1996 2942 2030 1996 4735 3979 1012 102 2053 5246 2031 2042 2275 2005 1996 4735 2030 2942 3572 1010 2021 17137 3051 2038 12254 2025 5905 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "12/02/2020 23:04:24 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "12/02/2020 23:04:24 - INFO - __main__ -   ***** Running evaluation:: Task : mrpc, Prefix : Previous Task (Continual) *****\n",
            "12/02/2020 23:04:24 - INFO - __main__ -     Num examples = 408\n",
            "12/02/2020 23:04:24 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 13/13 [00:03<00:00,  3.99it/s]\n",
            "12/02/2020 23:04:27 - INFO - __main__ -   ***** Eval results Previous Task (Continual) *****\n",
            "{'acc': 0.7181372549019608, 'f1': 0.8291233283803864, 'acc_and_f1': 0.7736302916411736}\n",
            "12/02/2020 23:04:27 - INFO - __main__ -    global_step = 156, average loss = 0.6207393554922862\n",
            "\n",
            "***** Accuracy Matrix *****\n",
            "\n",
            "[[0.5552853 0.6838235 0.534296 ]\n",
            " [0.5491398 0.8235294 0.4945848]\n",
            " [0.5207572 0.7181373 0.66787  ]]\n",
            "\n",
            "***** Transfer Matrix *****\n",
            "Future Transfer => Upper Triangular Matrix  ||  Backward Transfer => Lower Triangular Matrix\n",
            "\n",
            "[[ 0.        -0.1397059 -0.133574 ]\n",
            " [-0.0061455  0.        -0.1732852]\n",
            " [-0.0345281 -0.1053921  0.       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hp0FvZ6mK2u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}